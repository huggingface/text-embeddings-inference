# TEI Listwise Reranker êµ¬í˜„ ê°€ì´ë“œ
## LLM ê¸°ë°˜ ê°œë°œì„ ìœ„í•œ ì™„ì „í•œ ì½”ë“œ ì°¸ì¡° ë¬¸ì„œ

**ë²„ì „:** 1.4 (ìµœì¢… - ìŠ¹ì¸ë¨)
**ëŒ€ìƒ:** Text Embeddings Inference (TEI) - Jina v3 Listwise Reranker ì§€ì›
**ë°±ì—”ë“œ:** Candle (ìš°ì„ ìˆœìœ„), Python (ì°¸ì¡°ìš©ë§Œ)
**ê²€í†  ìƒíƒœ:** âœ… **ìŠ¹ì¸ë¨** - ë¸”ë¡œì»¤ í•´ê²°, ê³ ê°€ì¹˜ ê°œì„ ì‚¬í•­ ì ìš©, ë³‘í•© ìŠ¹ì¸

---

## ğŸ¯ í•µì‹¬ êµ¬í˜„ ì§€ì¹¨

**ì´ ê°€ì´ë“œë¥¼ êµ¬í˜„í•  ë•Œ ë°˜ë“œì‹œ ë”°ë¼ì•¼ í•  ê·œì¹™:**

### âš ï¸ ì„¸ì…˜ë‹¹ ì‘ì€ ë²”ìœ„ì˜ í•˜ìœ„ ì‘ì—…ì„ êµ¬í˜„í•˜ì„¸ìš”

1. **íŒŒì¼ì„ í¸ì§‘/ì¶”ê°€/ì‚­ì œí•œ í›„ ë°˜ë“œì‹œ ì‹¤í–‰:**
   ```bash
   cargo fmt && cargo clippy --all --all-targets --all-features
   ```

2. **í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° í†µê³¼í•  ë•Œê¹Œì§€ ë°˜ë³µ:**
   ```bash
   cargo test --all
   ```

3. **1, 2ë²ˆì„ ì™„ë£Œí•˜ê³  ê¸°ëŠ¥ì„ êµ¬í˜„í•œ í›„, ê° í•˜ìœ„ ì‘ì—…ì˜ ì²´í¬ë°•ìŠ¤ë¥¼ í‘œì‹œí•˜ì—¬ ì‘ì—… ì™„ë£Œë¥¼ ì•Œë¦¬ì„¸ìš”.**

### êµ¬í˜„ ìˆœì„œ
- Milestone 1ë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ ì§„í–‰
- ê° Milestoneì€ ë…ë¦½ì ìœ¼ë¡œ ì»´íŒŒì¼ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„ë¨
- ì´ì „ Milestoneì´ ì™„ë£Œë˜ì–´ì•¼ ë‹¤ìŒìœ¼ë¡œ ì§„í–‰ ê°€ëŠ¥

### ğŸ” í’ˆì§ˆ ê²€ì¦ ëª…ë ¹ì–´

**ì¤‘ìš”:** RouterëŠ” `http` ë˜ëŠ” `grpc` í”¼ì²˜ê°€ í•„ìš”í•˜ë¯€ë¡œ, workspace ì „ì²´ ê²€ì¦ì‹œ ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

```bash
# 1. í¬ë§·íŒ… (í•­ìƒ ë¨¼ì € ì‹¤í–‰)
cargo fmt

# 2. Candle ë°±ì—”ë“œ íŒ¨í‚¤ì§€ ë‹¨ë… ê²€ì¦
cargo clippy -p text-embeddings-backend-candle --no-deps -- --deny warnings

# 3. Workspace ì „ì²´ ê²€ì¦ (ì˜¬ë°”ë¥¸ ë°©ë²•)
cargo clippy --no-default-features --features candle,http --no-deps -- --deny warnings

# 4. ë¹Œë“œ í™•ì¸
cargo build -p text-embeddings-backend-candle

# âŒ ì˜ëª»ëœ ê²€ì¦ (router ì»´íŒŒì¼ ì‹¤íŒ¨)
# cargo clippy --no-default-features --features candle --no-deps -- --deny warnings
```

---

## ğŸ“‹ êµ¬í˜„ ì§„í–‰ ìƒí™©

- [x] **Milestone 1: ëª¨ë¸ ê°ì§€ ë° í•µì‹¬ íƒ€ì…** âœ…
  - [x] Detection logic with projector verification
  - [x] CLI parsing and AppState wiring
  - [x] Tests passing
  - Commits: 92febd3, f27e9a2, cecd5fb

- [x] **Milestone 2: í”„ë¡¬í”„íŠ¸ ë° í† í¬ë‚˜ì´ì œì´ì…˜ ë ˆì´ì–´** âœ…
  - [x] Prompt module (sanitize_input, build_jina_v3_prompt)
  - [x] Tokenization extensions (encode_listwise, truncate_texts)
  - [x] Module exports
  - Commit: 86da559

- [x] **Milestone 3: ë°±ì—”ë“œ ì¶”ìƒí™”** âœ…
  - [x] ListwiseBlockInput and ListwiseBlockOutput structs in backends/core
  - [x] Backend trait extended with embed_listwise_block() method (default Unsupported error)
  - [x] BackendCommand::EmbedListwise variant added with dispatch logic
  - [x] **Fixed DType::Float16 compilation error** (feature-gated Default/Display impl)
  - [x] Fixed tokenization type error in core (String â†’ &str conversion)
  - [x] Added unit tests for listwise types and default backend behavior (3 new tests)
  - [x] Fixed hf_hub sync API test (commented out - requires ureq feature)
  - [x] Fixed router test run() signature (added 9 listwise parameters)
  - **Tests: 25 passed** (3 backend-core + 11 core + 11 router), 0 failed
  - Note: Tokenizer configuration deferred to router layer (Milestone 5+)

- [x] **Milestone 4: Candle ë°±ì—”ë“œ êµ¬í˜„** âœ…
  - [x] Qwen3 hidden state API (forward_layers, forward_with_tensors)
  - [x] Projector layer (backends/candle/src/layers/projector.rs)
  - [x] LbnlReranker model (backends/candle/src/models/lbnl_reranker.rs)
  - [x] CandleBackend integration with projector weight detection
  - [x] Model trait implementation for LbnlReranker
  - [x] Module declarations and exports
  - [x] Fixed compilation errors (candle imports, tensor operations)
  - [x] Fixed clippy warnings (needless return, unused imports)
  - **Build: âœ… Successful** - `cargo build -p text-embeddings-backend-candle`
  - **Clippy:**
    - âœ… Package-level: `cargo clippy -p text-embeddings-backend-candle --no-deps -- --deny warnings` (PASS)
    - âœ… Workspace-level: `cargo clippy --no-default-features --features candle,http --no-deps -- --deny warnings` (PASS)
    - âš ï¸  Router dependency: Router requires `http` or `grpc` feature; candle-only (`--features candle`) fails at workspace level
  - **Tests: âš ï¸ Network-dependent** - Integration tests require HuggingFace model downloads (no network in env)
  - Note: Candle backend code compiles and passes all static checks; runtime tests deferred to environment with network access

- [x] **Milestone 5: ë¼ìš°í„° í†µí•© - íŠ¹ìˆ˜ í† í° ê²€ì¦** âœ…
  - [x] validate_special_tokens() function in core/src/tokenization.rs
  - [x] Validates embed_token count matches document count
  - [x] Validates rerank_token count is exactly 1
  - [x] Returns clear error messages for validation failures
  - [x] 4 unit tests (success, missing_embed, extra_rerank, no_rerank)
  - [x] All tests passing
  - [x] No clippy warnings
  - **Tests: 4 passed** (validation_tests module), 0 failed
  - Function accessible as `text_embeddings_core::tokenization::validate_special_tokens`

- [x] **Milestone 6: ë¼ìš°í„° í†µí•© - ìˆ˜í•™ ìœ í‹¸ë¦¬í‹°** âœ…
  - [x] Math utilities module (router/src/listwise/math.rs)
  - [x] cosine_similarity() with internal L2 normalization
  - [x] normalize() and normalize_new() for vector normalization
  - [x] weighted_average() for combining block embeddings
  - [x] add_scaled() for AXPY operations
  - [x] Epsilon stability (1e-8) for zero-norm protection
  - [x] Cosine result clamping to [-1, 1]
  - [x] Comprehensive error handling (dimension mismatches, empty vectors)
  - [x] 12 unit tests (orthogonal, parallel, antiparallel, edge cases)
  - [x] All tests passing
  - [x] No clippy warnings
  - **Tests: 12 passed**, 0 failed
  - Functions accessible as `router::listwise::math::*`
  - Normalization policy: L2 norm happens ONLY in cosine_similarity (modeling.py parity)

- [x] **Milestone 7: í ê²©ë¦¬ ë° Prometheus ë©”íŠ¸ë¦­** âœ…
  - [x] Queue isolation policy documented in router/src/listwise/mod.rs
  - [x] Shared worker queue design (BackendCommand::EmbedListwise from Milestone 3)
  - [x] No cross-request batching (privacy/accuracy guarantee)
  - [x] Prometheus metrics buckets configured in router/src/prometheus.rs
  - [x] Histogram buckets: tei_lbnl_ms_per_group (duration in ms)
  - [x] Histogram buckets: tei_lbnl_seq_tokens (sequence length)
  - [x] Histogram buckets: tei_lbnl_group_size (docs per block)
  - [x] Counter: tei_lbnl_block_timeout_total (timeout events, will be used in handler)
  - [x] All buckets properly registered with PrometheusBuilder
  - [x] No clippy warnings
  - **Tests: 23 passed**, 0 failed (router lib tests)
  - Metrics will be recorded in Milestone 8 handler implementation

- [x] **Milestone 8: ë¼ìš°í„° í•¸ë“¤ëŸ¬ êµ¬í˜„** âœ…
  - [x] rerank_listwise() HTTP handler in router/src/http/listwise_handler.rs
  - [x] Input validation (empty texts, max documents, max document length)
  - [x] Text truncation with modeling.py parity (query: 512, docs: 2048)
  - [x] Block construction algorithm (max 125 docs OR capacity exhaustion)
  - [x] **CRITICAL FIX**: Block weight calculation = max((1 + scores) / 2.0) âœ…
    - Previous: Used doc count (WRONG)
    - Current: Uses max normalized score from block (matches modeling.py line ~180)
  - [x] Zero-weight protection (fallback to equal weighting when total < 1e-6)
  - [x] Special token validation with BAD_REQUEST (400) error code âœ…
    - Previous: Returned INTERNAL_SERVER_ERROR (500) (WRONG)
    - Current: Returns BAD_REQUEST (400) for validation failures
  - [x] Weighted average query embedding aggregation
  - [x] Cosine similarity final scoring
  - [x] Prometheus metrics integration (tei_lbnl_ms_per_group, tei_lbnl_seq_tokens, tei_lbnl_group_size, tei_lbnl_block_timeout_total)
  - [x] Strategy dispatch (Auto/Pairwise/Listwise) via determine_strategy()
  - [x] AppState extended with tokenizer field
  - [x] Updated comments to match actual behavior
  - [x] **Random ordering implementation** âœ…
    - Uses combined tuple approach: (original_idx, doc, token_length)
    - ChaCha8Rng for cross-platform reproducibility
    - Seed support via config.random_seed
    - Maintains correct index mapping to req.texts
  - [x] **Block spill/shrink verification** âœ…
    - **VERIFIED: Does NOT exist in modeling.py**
    - Python code directly tokenizes without overflow checking
    - NO retry loop or spill logic in reference implementation
    - Decision: NOT implemented (avoiding feature creep, maintaining parity)
  - **Tests: 23 passed**, 0 failed
- [ ] **Milestone 9: End-to-End í†µí•©**
  - note: Milestone 9.3 (Infer integration) completed âœ…    Summary    Files Modified:   1. backends/src/lib.rs:     - Made backend_sender public     - Made BackendCommand enum public (required for public field)   2. core/src/infer.rs:     - Added embed_listwise_block() async method with backpressure-safe send().await     - Uses BackendCommand::EmbedListwise variant from Milestone 3     - Implements blocker B2 fix (avoids panic on full channel)
---

## ëª©ì°¨

1. [ê°œìš”](#1-ê°œìš”)
2. [í”„ë¡œì íŠ¸ êµ¬ì¡°](#2-í”„ë¡œì íŠ¸-êµ¬ì¡°)
3. [Milestone 1: ëª¨ë¸ ê°ì§€ ë° í•µì‹¬ íƒ€ì…](#milestone-1-ëª¨ë¸-ê°ì§€-ë°-í•µì‹¬-íƒ€ì…)
4. [Milestone 2: í”„ë¡¬í”„íŠ¸ ë° í† í¬ë‚˜ì´ì œì´ì…˜ ë ˆì´ì–´](#milestone-2-í”„ë¡¬í”„íŠ¸-ë°-í† í¬ë‚˜ì´ì œì´ì…˜-ë ˆì´ì–´)
5. [Milestone 3: ë°±ì—”ë“œ ì¶”ìƒí™”](#milestone-3-ë°±ì—”ë“œ-ì¶”ìƒí™”)
6. [Milestone 4: Candle ë°±ì—”ë“œ êµ¬í˜„](#milestone-4-candle-ë°±ì—”ë“œ-êµ¬í˜„)
7. [Milestone 5: ë¼ìš°í„° í†µí•© - íŠ¹ìˆ˜ í† í° ê²€ì¦](#milestone-5-ë¼ìš°í„°-í†µí•©---íŠ¹ìˆ˜-í† í°-ê²€ì¦)
8. [Milestone 6: ë¼ìš°í„° í†µí•© - ìˆ˜í•™ ìœ í‹¸ë¦¬í‹°](#milestone-6-ë¼ìš°í„°-í†µí•©---ìˆ˜í•™-ìœ í‹¸ë¦¬í‹°)
9. [Milestone 7: í ê²©ë¦¬ ë° Prometheus ë©”íŠ¸ë¦­](#milestone-7-í-ê²©ë¦¬-ë°-prometheus-ë©”íŠ¸ë¦­)
10. [Milestone 8: ë¼ìš°í„° í•¸ë“¤ëŸ¬ êµ¬í˜„](#milestone-8-ë¼ìš°í„°-í•¸ë“¤ëŸ¬-êµ¬í˜„)
11. [Milestone 9: End-to-End í†µí•©](#milestone-9-end-to-end-í†µí•©)
12. [ì˜ì¡´ì„±](#ì˜ì¡´ì„±--cargotoml)

---

## 1. ê°œìš”

ì´ ê°€ì´ë“œëŠ” TEIì—ì„œ Jina v3 listwise rerankingì„ êµ¬í˜„í•˜ê¸° ìœ„í•œ **ì™„ì „í•˜ê³  í”„ë¡œë•ì…˜ ì¤€ë¹„ê°€ ëœ ì½”ë“œ**ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ëª¨ë“  ì½”ë“œ ìŠ¤ë‹ˆí«ì€:

- âœ… **ì™„ì „íˆ ì»´íŒŒì¼ ê°€ëŠ¥** - ëª¨ë“  importì™€ ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨
- âœ… **íƒ€ì… ì•ˆì „** - ì ì ˆí•œ Rust ì–´ë…¸í…Œì´ì…˜
- âœ… **ì—£ì§€ ì¼€ì´ìŠ¤ ì¸ì‹** - ê²€ì¦ ë¡œì§ í¬í•¨
- âœ… **í…ŒìŠ¤íŠ¸ ì¤€ë¹„ ì™„ë£Œ** - ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì˜ˆì œ í¬í•¨
- âœ… **í†µí•© ì™„ë£Œ** - ì»´í¬ë„ŒíŠ¸ ì—°ê²° ë°©ë²• ì œì‹œ

### í•µì‹¬ ì›ì¹™

1. **TODO ì—†ìŒ**: ëª¨ë“  ì½”ë“œëŠ” êµ¬í˜„ ì¤€ë¹„ ì™„ë£Œ
2. **ì „ì²´ ì»¨í…ìŠ¤íŠ¸**: ê° ìŠ¤ë‹ˆí«ì— í•„ìš”í•œ import í¬í•¨
3. **ì—ëŸ¬ ì²˜ë¦¬**: ëª¨ë“  Result íƒ€ì… ì ì ˆíˆ ì •ì˜
4. **ê²€ì¦**: ì…ë ¥ sanitization ë° ê²½ê³„ ì²´í¬
5. **Python ì°¸ì¡°ì™€ ë™ë“±**: `modeling.py`ì™€ ì¼ì¹˜

### ê²€í†  ìŠ¹ì¸ ë…¸íŠ¸

- âœ… ì™¸ë¶€ ê²€í† ì—ì„œ ì „ì²´ ì•„í‚¤í…ì²˜ ìŠ¹ì¸, í•„ìˆ˜ ìˆ˜ì •ì‚¬í•­ ëª©ë¡ ì œê³µ
- âœ… ì´ ë²„ì „ì€ ê²€í† ì—ì„œ ì§€ì ëœ ëª¨ë“  ë¸”ë¡œì»¤ í¬í•¨ (crate ì´ë¦„, Qwen3 hidden-state API, handler score ê³„ì‚°, projector normalization ì •ì±…, tokenization ì•ˆì „ì„±)
- âœ… Should-fix ê°€ì´ë“œ (projector ê°ì§€ í´ë°±, handler ë°˜í™˜ íƒ€ì…, random seeding)ë„ ê´€ë ¨ Milestoneì— ë°˜ì˜
- âœ… ê²€í† ì˜ Nit (ë¬¸ì„œ í‘œí˜„, ë©”íŠ¸ë¦­ ëª…í™•í™”)ë„ í•´ë‹¹ë˜ëŠ” ê³³ì— ë°˜ì˜

### ì „ì—­ ì•„í‚¤í…ì²˜ ì •ì±…

**ì •ê·œí™” ì •ì±…:**
ëª¨ë“  L2 ì •ê·œí™”ëŠ” **ì˜¤ì§** ë¼ìš°í„°ì˜ `cosine_similarity()` í•¨ìˆ˜ì—ì„œë§Œ ë°œìƒí•©ë‹ˆë‹¤ (Milestone 6 ì°¸ì¡°). Projectorì™€ ë°±ì—”ë“œëŠ” ì •ê·œí™”ë˜ì§€ ì•Šì€ ì„ë² ë”©ì„ ë°˜í™˜í•©ë‹ˆë‹¤. ì´ëŠ” `modeling.py`ì™€ ì¼ì¹˜í•˜ë©°, ì—¬ê¸°ì„œ `normalize()`ëŠ” `compute_scores()` ë‚´ì—ì„œ í˜¸ì¶œë©ë‹ˆë‹¤. ë°±ì—”ë“œì—ì„œ ì •ê·œí™”í•˜ë©´ ì´ì¤‘ ì •ê·œí™”ê°€ ë°œìƒí•©ë‹ˆë‹¤!

**ê·¼ê±°:** í•œ ê³³ì—ì„œ ì •ê·œí™”ë¥¼ ì¤‘ì•™í™”í•¨ìœ¼ë¡œì¨ ì´ì¤‘ ì •ê·œí™”ë¡œ ì¸í•œ ë¯¸ë¬˜í•œ ë²„ê·¸ë¥¼ ë°©ì§€í•˜ê³  Python ì°¸ì¡° êµ¬í˜„ê³¼ ì •í™•í•œ ìˆ˜ì¹˜ì  ë™ë“±ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.

---

## 2. í”„ë¡œì íŠ¸ êµ¬ì¡°

```
text-embeddings-inference/
â”œâ”€â”€ backends/
â”‚   â”œâ”€â”€ candle/
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â”œâ”€â”€ layers/
â”‚   â”‚       â”‚   â””â”€â”€ projector.rs          # ì‹ ê·œ: MLP Projector
â”‚   â”‚       â”œâ”€â”€ models/
â”‚   â”‚       â”‚   â”œâ”€â”€ qwen3.rs              # ìˆ˜ì •: hidden state ì¶”ì¶œ ì¶”ê°€
â”‚   â”‚       â”‚   â””â”€â”€ lbnl_reranker.rs      # ì‹ ê·œ: Last but not Late Interaction ëª¨ë¸
â”‚   â”‚       â””â”€â”€ lib.rs                    # ìˆ˜ì •: LBNL ì§€ì› ì¶”ê°€
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â””â”€â”€ lib.rs                    # ìˆ˜ì •: Backend traitì— listwise hook ì¶”ê°€
â”‚   â””â”€â”€ src/
â”‚       â””â”€â”€ lib.rs                        # ìˆ˜ì •: ModelType enum
â”œâ”€â”€ core/
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ prompt.rs                     # ì‹ ê·œ: í”„ë¡¬í”„íŠ¸ ë¹Œë”©
â”‚       â”œâ”€â”€ detection.rs                  # ì‹ ê·œ: ëª¨ë¸ ê°ì§€ (ìˆœí™˜ ì˜ì¡´ì„± ë°©ì§€)
â”‚       â”œâ”€â”€ tokenization.rs               # ìˆ˜ì •: Listwise ì¸ì½”ë”©
â”‚       â””â”€â”€ infer.rs                      # ìˆ˜ì •: Listwise dispatch
â”œâ”€â”€ router/
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ lib.rs                        # ìˆ˜ì •: ê°ì§€ ë¡œì§
â”‚       â”œâ”€â”€ strategy.rs                   # ì‹ ê·œ: Strategy íƒ€ì… ì •ì˜
â”‚       â”œâ”€â”€ listwise/
â”‚       â”‚   â”œâ”€â”€ mod.rs                    # ì‹ ê·œ: Listwise orchestration
â”‚       â”‚   â””â”€â”€ math.rs                   # ì‹ ê·œ: ë²¡í„° ìˆ˜í•™ ìœ í‹¸ë¦¬í‹°
â”‚       â”œâ”€â”€ http/
â”‚       â”‚   â””â”€â”€ server.rs                 # ìˆ˜ì •: Listwise í•¸ë“¤ëŸ¬
â”‚       â””â”€â”€ prometheus.rs                 # ìˆ˜ì •: ë©”íŠ¸ë¦­
â””â”€â”€ Cargo.toml                            # ìˆ˜ì •: ì˜ì¡´ì„±
```

> **Crate ì´ë¦„ ê·œì¹™:** ì•„ë˜ ì½”ë“œ ìŠ¤ë‹ˆí«ì€ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ crate ì´ë¦„ì´ `router`, `text_embeddings_core`, `text_embeddings_backend_core`, `text_embeddings_backend_candle`ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤ (TEIì˜ ê¸°ì¡´ íŒ¨í„´ê³¼ ì¼ì¹˜). ìŠ¤ë‹ˆí«ì„ ë³µì‚¬í•˜ê¸° ì „ì— `Cargo.toml`ì˜ `package.name` í•­ëª©ì„ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”.

---

## Milestone 1: ëª¨ë¸ ê°ì§€ ë° í•µì‹¬ íƒ€ì…

### 1.1 ë°±ì—”ë“œ ì½”ì–´ - ModelType (ë³€ê²½ ì—†ìŒ)

**íŒŒì¼:** `/backends/core/src/lib.rs`
**ìœ„ì¹˜:** ê¸°ì¡´ enum ì •ì˜ì— ì¶”ê°€

```rust
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::{Path, PathBuf};
use anyhow::{anyhow, Context, Result};
use tokenizers::Tokenizer;

// ì£¼ì˜: backends/coreì˜ ModelType enumì€ ë³€ê²½ë˜ì§€ ì•ŠìŒ
// Listwise ê¸°ëŠ¥ì€ ModelKindë¥¼ í†µí•´ ë¼ìš°í„° ìˆ˜ì¤€ì—ì„œ ê°ì§€ë¨
// ModelKind::ListwiseRerankerëŠ” router/src/lib.rs ì°¸ì¡°
```

### 1.1.1 ë¼ìš°í„° Strategy íƒ€ì…

**íŒŒì¼:** `router/src/strategy.rs` (ì‹ ê·œ íŒŒì¼)

```rust
//! Reranking strategy íƒ€ì… ë° CLI ì¸ì íŒŒì‹±
//!
//! ì´ ëª¨ë“ˆì€ listwise vs pairwise reranking ë™ì‘ì„ ì œì–´í•˜ê¸° ìœ„í•œ
//! ë¼ìš°í„° ìˆ˜ì¤€ì˜ enumì„ í¬í•¨í•©ë‹ˆë‹¤.

use anyhow::{anyhow, Result};
use serde::{Deserialize, Serialize};

/// ëŸ°íƒ€ì„ reranking strategy (ìš”ì²­ ì‹œì ì— ê²°ì •ë¨)
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum RerankStrategy {
    Pairwise,
    Listwise,
}

/// Reranker ì„ íƒì„ ìœ„í•œ CLI ëª¨ë“œ
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum RerankMode {
    Auto,
    Pairwise,
    Listwise,
}

impl std::str::FromStr for RerankMode {
    type Err = anyhow::Error;

    fn from_str(s: &str) -> Result<Self> {
        match s.to_lowercase().as_str() {
            "auto" => Ok(Self::Auto),
            "pairwise" => Ok(Self::Pairwise),
            "listwise" => Ok(Self::Listwise),
            _ => Err(anyhow!(
                "Invalid reranker mode: {}. Valid values: auto, pairwise, listwise",
                s
            )),
        }
    }
}

/// Listwise ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¬¸ì„œ ìˆœì„œ strategy
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum RerankOrdering {
    Input,
    Random,
}

impl std::str::FromStr for RerankOrdering {
    type Err = anyhow::Error;

    fn from_str(s: &str) -> Result<Self> {
        match s.to_lowercase().as_str() {
            "input" => Ok(Self::Input),
            "random" => Ok(Self::Random),
            _ => Err(anyhow!(
                "Invalid rerank ordering: {}. Valid values: input, random",
                s
            )),
        }
    }
}
```

### 1.2 ëª¨ë¸ ê°ì§€ ë¡œì§ (ìˆœí™˜ ì˜ì¡´ì„± ë°©ì§€)

**íŒŒì¼:** `core/src/detection.rs` (ì‹ ê·œ - ê³µìœ  ê°ì§€ ë¡œì§)

ì´ ëª¨ë“ˆì€ ë¼ìš°í„° â†” candle ìˆœí™˜ ì˜ì¡´ì„±ì„ í”¼í•˜ê¸° ìœ„í•´ `core`ì— ìœ„ì¹˜í•©ë‹ˆë‹¤.

```rust
//! ë¼ìš°í„°ì™€ ë°±ì—”ë“œ ê°„ ê³µìœ ë˜ëŠ” ëª¨ë¸ ê°ì§€ ìœ í‹¸ë¦¬í‹°
//!
//! ì¤‘ìš”: ì´ ëª¨ë“ˆì€ ìˆœí™˜ ì˜ì¡´ì„±ì„ í”¼í•˜ê¸° ìœ„í•´ `core`ì— ìˆìŠµë‹ˆë‹¤.
//! `router`ì™€ `backends/candle` ëª¨ë‘ ì—¬ê¸°ì„œ ì•ˆì „í•˜ê²Œ importí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

use anyhow::{Context, Result};
use serde_json::Value;
use std::path::Path;
use tokenizers::Tokenizer;

/// ëª¨ë¸ ì¢…ë¥˜ ë¶„ë¥˜
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ModelKind {
    Embedding,
    SequenceClassifier,
    ListwiseReranker,
}

/// ëª¨ë¸ì´ LBNL ì„œëª…(projector ê°€ì¤‘ì¹˜ + íŠ¹ìˆ˜ í† í°)ì„ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸
/// PUBLIC: ë¼ìš°í„° ê°ì§€ ë° candle ë°±ì—”ë“œ ì´ˆê¸°í™” ëª¨ë‘ì—ì„œ ì‚¬ìš©ë¨
pub fn has_lbnl_signature(model_path: &Path, tokenizer: &Tokenizer) -> Result<bool> {
    // ì²´í¬ 1: ì•„í‚¤í…ì²˜ (Qwen3 ê¸°ë°˜)
    if !is_qwen_architecture(model_path)? {
        return Ok(false);
    }

    // ì²´í¬ 2: Projector ê°€ì¤‘ì¹˜ ì¡´ì¬
    if !has_projector_weights(model_path)? {
        return Ok(false);
    }

    // ì²´í¬ 3: íŠ¹ìˆ˜ í† í° ì¡´ì¬
    if !has_special_tokens(tokenizer)? {
        return Ok(false);
    }

    Ok(true)
}

/// ì•„í‚¤í…ì²˜ê°€ Qwen3 ê¸°ë°˜ì¸ì§€ í™•ì¸
fn is_qwen_architecture(model_path: &Path) -> Result<bool> {
    let config_path = model_path.join("config.json");
    let config_str = std::fs::read_to_string(config_path)
        .context("Failed to read config.json")?;
    let config: Value = serde_json::from_str(&config_str)
        .context("Failed to parse config.json")?;

    // architectures í•„ë“œ í™•ì¸
    if let Some(arch_array) = config.get("architectures").and_then(|v| v.as_array()) {
        for arch in arch_array {
            if let Some(arch_str) = arch.as_str() {
                if matches!(
                    arch_str,
                    "QwenForCausalLM" | "Qwen3ForCausalLM" | "JinaForRanking"
                ) {
                    return Ok(true);
                }
            }
        }
    }

    // í´ë°±ìœ¼ë¡œ model_type í•„ë“œ í™•ì¸ (qwen3ë§Œ)
    if let Some(model_type) = config.get("model_type").and_then(|v| v.as_str()) {
        if model_type == "qwen3" {
            return Ok(true);
        }
    }

    Ok(false)
}

/// Projector ê°€ì¤‘ì¹˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (bias ì—†ì´)
fn has_projector_weights(model_path: &Path) -> Result<bool> {
    // ìš°ì„ ìˆœìœ„ 1: index.json í™•ì¸ (ìƒ¤ë”©ëœ ëª¨ë¸)
    let index_path = model_path.join("model.safetensors.index.json");
    if index_path.exists() {
        return check_projector_in_index(&index_path);
    }

    // ìš°ì„ ìˆœìœ„ 2: ë‹¨ì¼ safetensors íŒŒì¼ í—¤ë” í™•ì¸ (ìƒ¤ë”©ë˜ì§€ ì•Šì€ ëª¨ë¸ì— ì¤‘ìš”)
    let single_file = model_path.join("model.safetensors");
    if single_file.exists() {
        return check_projector_in_safetensors(&single_file);
    }

    // ìš°ì„ ìˆœìœ„ 3: ë¹„ì •ìƒ ë ˆì´ì•„ì›ƒ í´ë°± (pytorch_model.bin ë“±)
    let mut has_proj0 = false;
    let mut has_proj2 = false;
    for entry in std::fs::read_dir(model_path)? {
        let path = entry?.path();
        if !path.is_file() {
            continue;
        }
        let name = path.file_name().and_then(|s| s.to_str()).unwrap_or("");
        if name.contains("projector.0.weight") {
            has_proj0 = true;
        }
        if name.contains("projector.2.weight") {
            has_proj2 = true;
        }
    }
    Ok(has_proj0 && has_proj2)
}

/// ë‹¨ì¼ safetensors íŒŒì¼ì—ì„œ í—¤ë”ë¥¼ ì½ì–´ projector í™•ì¸
///
/// âš ï¸ **í•„ìˆ˜ ìˆ˜ì • 2: ë©€í‹° GB ëª¨ë¸ì„ ìœ„í•œ ë©”ëª¨ë¦¬ ë§µ I/O**
/// ë©€í‹° GB ëª¨ë¸ íŒŒì¼ì— std::fs::read() ì‚¬ìš©ì‹œ ë©”ëª¨ë¦¬ í­ë°œ ë°œìƒ.
/// ë©”ëª¨ë¦¬ ë§µ I/OëŠ” ì œë¡œ ì¹´í”¼ í—¤ë” íŒŒì‹± ì œê³µ.
fn check_projector_in_safetensors(path: &Path) -> Result<bool> {
    use safetensors::SafeTensors;
    use std::fs::File;
    use memmap2::MmapOptions;

    // í•„ìˆ˜ ìˆ˜ì • 2: ì „ì²´ íŒŒì¼ì„ RAMì— ì½ëŠ” ëŒ€ì‹  ë©”ëª¨ë¦¬ ë§µ íŒŒì¼ ì‚¬ìš©
    // 10GB ëª¨ë¸ì˜ ê²½ìš° std::fs::readëŠ” í—¤ë” íŒŒì‹±ë§Œì„ ìœ„í•´ 10GB RAM í• ë‹¹!
    // mmapì€ ì œë¡œ ì¹´í”¼ ì•¡ì„¸ìŠ¤ ì œê³µ - í—¤ë” í˜ì´ì§€ë§Œ ì‹¤ì œë¡œ ë¡œë“œë¨
    let file = File::open(path)
        .context("Failed to open safetensors file")?;

    let mmap = unsafe {
        MmapOptions::new()
            .map(&file)
            .context("Failed to memory-map safetensors file")?
    };

    // SafeTensors::deserializeëŠ” í—¤ë”ë§Œ ì½ìŒ (ì²˜ìŒ ëª‡ KB)
    // mmap ì‚¬ìš©ì‹œ ì „ì²´ íŒŒì¼ ë¡œë“œí•˜ì§€ ì•ŠìŒ - í—¤ë” í˜ì´ì§€ë§Œ í˜ì´ì§€ ì¸
    let tensors = SafeTensors::deserialize(&mmap)
        .context("Failed to parse safetensors header")?;

    // í•„ìˆ˜ projector ê°€ì¤‘ì¹˜ í™•ì¸
    let has_w0 = tensors.names().any(|n| n == "projector.0.weight");
    let has_w2 = tensors.names().any(|n| n == "projector.2.weight");

    // ì¤‘ìš”: bias ì—†ìŒ í™•ì¸ (bias=False ìš”êµ¬ì‚¬í•­)
    let has_b0 = tensors.names().any(|n| n == "projector.0.bias");
    let has_b2 = tensors.names().any(|n| n == "projector.2.bias");

    if has_b0 || has_b2 {
        // âš ï¸ ê°•ë ¥ ê¶Œì¥: ì§„ë‹¨ ì •ë³´ê°€ í¬í•¨ëœ í–¥ìƒëœ ì—ëŸ¬ ë©”ì‹œì§€
        let sample_keys: Vec<_> = tensors.names().take(10).collect();
        tracing::warn!(
            "Projector bias detected in {:?} (Jina v3 requires bias=False). \
             Model may be incompatible. Sample keys: {:?}",
            path, sample_keys
        );
        return Ok(false);
    }

    if !has_w0 || !has_w2 {
        // âš ï¸ ê°•ë ¥ ê¶Œì¥: ê°€ì¤‘ì¹˜ ëˆ„ë½ì‹œ ì§„ë‹¨ ì •ë³´ ë¡œê¹…
        let sample_keys: Vec<_> = tensors.names().take(10).collect();
        tracing::debug!(
            "Projector weights not found in {:?}. \
             Looking for 'projector.0.weight' and 'projector.2.weight'. \
             Sample keys: {:?}",
            path, sample_keys
        );
    }

    Ok(has_w0 && has_w2)
}

fn check_projector_in_index(index_path: &Path) -> Result<bool> {
    let index_str = std::fs::read_to_string(index_path)?;
    let index: Value = serde_json::from_str(&index_str)?;

    let weight_map = index
        .get("weight_map")
        .and_then(|v| v.as_object())
        .context("Invalid weight_map in index")?;

    let has_proj0_weight = weight_map.contains_key("projector.0.weight");
    let has_proj2_weight = weight_map.contains_key("projector.2.weight");
    let has_proj0_bias = weight_map.contains_key("projector.0.bias");
    let has_proj2_bias = weight_map.contains_key("projector.2.bias");

    Ok(has_proj0_weight && has_proj2_weight && !has_proj0_bias && !has_proj2_bias)
}

/// í† í¬ë‚˜ì´ì €ê°€ listwise rerankingì„ ìœ„í•œ íŠ¹ìˆ˜ í† í°ì„ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸
fn has_special_tokens(tokenizer: &Tokenizer) -> Result<bool> {
    let embed_token_id = tokenizer.token_to_id("<|embed_token|>");
    let rerank_token_id = tokenizer.token_to_id("<|rerank_token|>");

    Ok(embed_token_id.is_some() && rerank_token_id.is_some())
}

/// Listwise ìš°ì„ ìˆœìœ„ë¡œ ëª¨ë¸ ì¢…ë¥˜ ê°ì§€
///
/// ì¤‘ìš”: ê°ì§€ ì‹¤íŒ¨ ì²˜ë¦¬
/// - íŠ¹ìˆ˜ í† í° ì¡´ì¬í•˜ì§€ë§Œ projector ê°ì§€ ì‹¤íŒ¨ â†’ WARNING ë¡œê¹…, pairwiseë¡œ í´ë°±
/// - LBNL ëª¨ë¸ì˜ ì˜ëª»ëœ ë¼ìš°íŒ… ë°©ì§€
/// - ë°±ì—”ë“œëŠ” ê¶Œí•œìˆëŠ” ì²´í¬ë¥¼ ìˆ˜í–‰í•˜ë©° ê°€ì¤‘ì¹˜ê°€ ìœ íš¨í•˜ë©´ ì—¬ì „íˆ LBNLë¡œ ë¡œë“œ ê°€ëŠ¥
pub fn detect_model_kind(model_path: &Path, tokenizer: &Tokenizer) -> Result<ModelKind> {
    // ë” ë‚˜ì€ ì—ëŸ¬ ë³´ê³ ë¥¼ ìœ„í•´ ì»´í¬ë„ŒíŠ¸ë¥¼ ë…ë¦½ì ìœ¼ë¡œ í™•ì¸
    let has_qwen_arch = is_qwen_architecture(model_path).unwrap_or(false);
    let has_projector = has_projector_weights(model_path).unwrap_or(false);
    let has_tokens = has_special_tokens(tokenizer).unwrap_or(false);

    // ìš°ì„ ìˆœìœ„ 1: Listwise reranker (ëª¨ë“  ì¡°ê±´ì´ trueì—¬ì•¼ í•¨)
    if has_qwen_arch && has_projector && has_tokens {
        tracing::info!(
            "âœ“ Detected ListwiseReranker: arch=qwen3, projector=yes, tokens=yes"
        );
        return Ok(ModelKind::ListwiseReranker);
    }

    // ë¶€ë¶„ ê°ì§€: ìƒì„¸í•œ ê²½ê³  ë¡œê¹…
    if has_tokens && !has_projector {
        tracing::warn!(
            "âš  Model has listwise special tokens but NO projector weights detected. \
             Falling back to pairwise mode. This may be a detection error - \
             check model files or use --reranker-mode listwise to force. \
             Detection: arch={}, projector={}, tokens={}",
            has_qwen_arch, has_projector, has_tokens
        );
    } else if has_projector && !has_tokens {
        tracing::warn!(
            "âš  Model has projector weights but NO listwise special tokens. \
             Falling back to pairwise mode. Verify tokenizer_config.json. \
             Detection: arch={}, projector={}, tokens={}",
            has_qwen_arch, has_projector, has_tokens
        );
    }

    // ìš°ì„ ìˆœìœ„ 2: Sequence classifier (ê¸°ì¡´ ë¡œì§)
    if is_sequence_classifier(model_path)? {
        tracing::info!("âœ“ Detected SequenceClassifier model");
        return Ok(ModelKind::SequenceClassifier);
    }

    // ê¸°ë³¸ê°’: Embedding
    tracing::info!("âœ“ Detected Embedding model (default)");
    Ok(ModelKind::Embedding)
}

fn is_sequence_classifier(model_path: &Path) -> Result<bool> {
    let config_path = model_path.join("config.json");
    let config_str = std::fs::read_to_string(config_path)?;
    let config: Value = serde_json::from_str(&config_str)?;

    // id2label í™•ì¸ (classifier ì„œëª…)
    Ok(config.get("id2label").is_some())
}

/// CLI ëª¨ë“œì™€ ê°ì§€ëœ ëª¨ë¸ ì¢…ë¥˜ë¡œë¶€í„° ëŸ°íƒ€ì„ strategy ê²°ì •
///
/// âš ï¸ **ë¸”ë¡œì»¤ ìˆ˜ì •: ì˜ëª»ëœ ëª¨ë“œ ì¡°í•© ê±°ë¶€**
/// ì´ í•¨ìˆ˜ëŠ” ì´ì œ modeì™€ model_kindê°€ í˜¸í™˜ë˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
/// Listwise ì „ìš© ëª¨ë¸(LBNL)ì€ embed()/predict() ì¸í„°í˜ì´ìŠ¤ë¥¼
/// êµ¬í˜„í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ pairwise ëª¨ë“œì—ì„œ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
pub fn determine_strategy(mode: &RerankMode, kind: &ModelKind) -> Result<RerankStrategy> {
    use crate::strategy::RerankMode;
    
    match (mode, kind) {
        // Auto ëª¨ë“œ: ëª¨ë¸ ê¸°ëŠ¥ì— ë”°ë¼ ì ì ˆí•œ strategy ì„ íƒ
        (RerankMode::Auto, ModelKind::ListwiseReranker) => Ok(RerankStrategy::Listwise),
        (RerankMode::Auto, _) => Ok(RerankStrategy::Pairwise),

        // ë¸”ë¡œì»¤ ìˆ˜ì •: listwise ì „ìš© ëª¨ë¸ì— ëŒ€í•´ pairwise ëª¨ë“œ ëª…ì‹œì ìœ¼ë¡œ ê±°ë¶€
        // LbnlRerankerëŠ” embed()/predict() êµ¬í˜„í•˜ì§€ ì•ŠìŒ - ëŸ°íƒ€ì„ 5xx ë°œìƒ
        (RerankMode::Pairwise, ModelKind::ListwiseReranker) => Err(anyhow!(
            "This model only supports listwise reranking. \
             Use --reranker-mode auto or --reranker-mode listwise."
        )),
        (RerankMode::Pairwise, _) => Ok(RerankStrategy::Pairwise),

        // Listwise ëª¨ë“œ: ëª¨ë¸ì´ ì§€ì›í•˜ëŠ” ê²½ìš°ì—ë§Œ í—ˆìš©
        (RerankMode::Listwise, ModelKind::ListwiseReranker) => Ok(RerankStrategy::Listwise),
        (RerankMode::Listwise, kind) => Err(anyhow!(
            "Model kind {:?} does not support listwise reranking. \
             Model must have projector weights and special tokens. \
             Use --reranker-mode auto or --reranker-mode pairwise.",
            kind
        )),
    }
}
```

**íŒŒì¼:** `core/src/lib.rs`
**ìœ„ì¹˜:** ëª¨ë“ˆ export ë° public íƒ€ì… ì¬export ì¶”ê°€

âš ï¸ **í•„ìˆ˜ ìˆ˜ì • 1: TOKENIZATION ëª¨ë“ˆ EXPORT ì¶”ê°€**

```rust
pub mod detection;     // ì‹ ê·œ: ê³µìœ  ê°ì§€ ìœ í‹¸ë¦¬í‹°
pub mod prompt;        // ì‹ ê·œ: í”„ë¡¬í”„íŠ¸ ë¹Œë”©
pub mod tokenization;  // ì‹ ê·œ: í† í¬ë‚˜ì´ì œì´ì…˜ í—¬í¼ (ì¤‘ìš”: ë¼ìš°í„°ì—ì„œ í•„ìš”!)
// ... ê¸°ì¡´ ëª¨ë“ˆë“¤

// ì¤‘ìš”: ë” ì‰¬ìš´ importë¥¼ ìœ„í•´ detection íƒ€ì… ì¬export
pub use detection::{ModelKind, detect_model_kind, has_lbnl_signature, determine_strategy};
```

> **ì™œ ì¤‘ìš”í•œê°€:** ë¼ìš°í„° ì½”ë“œëŠ” `text_embeddings_core::tokenization::{encode_listwise, truncate_texts, validate_special_tokens}`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ exportê°€ ì—†ìœ¼ë©´ "module not found" ì—ëŸ¬ë¡œ ì»´íŒŒì¼ ì‹¤íŒ¨í•©ë‹ˆë‹¤.

**íŒŒì¼:** `router/src/lib.rs`
**ìœ„ì¹˜:** coreì—ì„œ import (ì—¬ê¸°ì„œ ModelKind ì¬ì •ì˜í•˜ì§€ ë§ ê²ƒ)

```rust
// ì¤‘ìš”: coreì—ì„œ ModelKind import, ë¼ìš°í„°ì—ì„œ ì •ì˜í•˜ì§€ ë§ ê²ƒ
use text_embeddings_core::detection::{
    ModelKind, detect_model_kind, has_lbnl_signature, determine_strategy
};

// âŒ ë¡œì»¬ ModelKind enum ì •ì˜ ì œê±° - coreì—ë§Œ ì¡´ì¬
```

### 1.4 AppState í™•ì¥

**íŒŒì¼:** `router/src/lib.rs`
**ìœ„ì¹˜:** listwise ì„¤ì •ì„ ìœ„í•œ ìƒˆ struct ì¶”ê°€

```rust
use std::sync::Arc;
use crate::strategy::{RerankMode, RerankOrdering};

/// Listwise reranking ì„¤ì •
#[derive(Debug, Clone)]
pub struct ListwiseConfig {
    pub max_docs_per_pass: usize,
    pub ordering: RerankOrdering,
    pub instruction: Option<String>,
    pub payload_limit_bytes: usize,
    pub block_timeout_ms: u64,
    pub random_seed: Option<u64>,
    pub max_documents_per_request: usize,
    pub max_document_length_bytes: usize,
}

impl Default for ListwiseConfig {
    fn default() -> Self {
        Self {
            max_docs_per_pass: 125,
            ordering: RerankOrdering::Input,
            instruction: None,
            payload_limit_bytes: 2_000_000,
            block_timeout_ms: 30_000,
            random_seed: None,
            max_documents_per_request: 1_000,
            max_document_length_bytes: 102_400,
        }
    }
}

/// í™•ì¥ëœ ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒíƒœ
#[derive(Clone)]
pub struct AppState {
    pub infer: Arc<Infer>,
    pub info: Arc<Info>,
    pub model_kind: ModelKind,
    pub reranker_mode: RerankMode,
    pub listwise_config: Arc<ListwiseConfig>,
}

// ì£¼ì˜: Info.max_input_lengthëŠ” í† í¬ë‚˜ì´ì €/ëª¨ë¸ ì„¤ì •ì— ì˜í•´ ê²°ì •ë¨
// RoPE scalingì„ ì‚¬ìš©í•˜ëŠ” Qwen3ì˜ ê²½ìš° ë‹¤ìŒ ë²”ìœ„ì¼ ìˆ˜ ìˆìŒ:
// - ê¸°ë³¸: 32K í† í° (Qwen3-0.6B ê¸°ë³¸ê°’)
// - í™•ì¥: 128K+ í† í° (config.jsonì— rope_scaling í¬í•¨)
// 8K/16K ì œí•œì„ ê°€ì •í•˜ì§€ ë§ ê²ƒ - ëŸ°íƒ€ì„ì— ì‹¤ì œ ëª¨ë¸ ì„¤ì • í™•ì¸

impl AppState {
    pub fn new(
        infer: Infer,
        info: Info,
        model_kind: ModelKind,
        reranker_mode: RerankMode,
        listwise_config: ListwiseConfig,
    ) -> Self {
        Self {
            infer: Arc::new(infer),
            info: Arc::new(info),
            model_kind,
            reranker_mode,
            listwise_config: Arc::new(listwise_config),
        }
    }

    /// í˜„ì¬ ìš”ì²­ì— ëŒ€í•œ strategy ê²°ì •
    pub fn determine_strategy(&self) -> Result<RerankStrategy> {
        determine_strategy(&self.reranker_mode, &self.model_kind)
    }
}
```

---

## Milestone 2: í”„ë¡¬í”„íŠ¸ ë° í† í¬ë‚˜ì´ì œì´ì…˜ ë ˆì´ì–´

### 2.1 í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆ

**íŒŒì¼:** `core/src/prompt.rs` (ì‹ ê·œ)

```rust
//! Jina v3 listwise rerankerë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ë¹Œë”©
//!
//! ì´ ëª¨ë“ˆì€ Python ì°¸ì¡° êµ¬í˜„ì˜ ì •í™•í•œ í…œí”Œë¦¿ì„ ë”°ë¥´ëŠ”
//! í”„ë¡¬í”„íŠ¸ êµ¬ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.

/// í”„ë¡¬í”„íŠ¸ ì£¼ì…ì„ ì¼ìœ¼í‚¬ ìˆ˜ ìˆëŠ” íŠ¹ìˆ˜ í† í° ì œê±°í•˜ì—¬ ì…ë ¥ í…ìŠ¤íŠ¸ sanitize
///
/// hidden state ì¶”ì¶œì„ ë°©í•´í•  ìˆ˜ ìˆëŠ” ë‘ ê°œì˜ ì„ë² ë”© ê´€ë ¨ í† í°ë§Œ ì œê±°í•©ë‹ˆë‹¤.
/// ì±„íŒ… í˜•ì‹ í† í°(<|im_start|>, <|im_end|>)ì€ ì •ìƒì ì¸ ì‚¬ìš©ì ì½˜í…ì¸ ì˜
/// ì¼ë¶€ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.
pub fn sanitize_input(text: &str) -> String {
    text.replace("<|embed_token|>", "")
        .replace("<|rerank_token|>", "")
}

/// Python ì°¸ì¡° í…œí”Œë¦¿ì„ ì •í™•íˆ ë”°ë¥´ëŠ” Jina v3 LBNL í”„ë¡¬í”„íŠ¸ ë¹Œë“œ
///
/// í…œí”Œë¦¿ êµ¬ì¡°:
/// 1. System ë©”ì‹œì§€ (ì—­í•  ì •ì˜)
/// 2. User ë©”ì‹œì§€:
///    - ë¬¸ì„œ ê°œìˆ˜ê°€ í¬í•¨ëœ ì‘ì—… ì„¤ëª…
///    - ì„ íƒì  instruction ë¸”ë¡
///    - <|embed_token|> ë§ˆì»¤ê°€ ìˆëŠ” Passages
///    - <|rerank_token|> ë§ˆì»¤ê°€ ìˆëŠ” Query ë¸”ë¡
/// 3. Thinking placeholderê°€ ìˆëŠ” Assistant ë©”ì‹œì§€
///
/// # ì¸ì
/// * `query` - ê²€ìƒ‰ ì¿¼ë¦¬ ë¬¸ìì—´ (sanitizeë¨)
/// * `docs` - ìˆœìœ„ë¥¼ ë§¤ê¸¸ ë¬¸ì„œ ë¬¸ìì—´ë“¤ (sanitizeë¨)
/// * `instruction` - ì„ íƒì  ì¶”ê°€ instruction
///
/// # ë°˜í™˜
/// í† í¬ë‚˜ì´ì œì´ì…˜ ì¤€ë¹„ê°€ ëœ ì™„ì „í•œ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
pub fn build_jina_v3_prompt(
    query: &str,
    docs: &[&str],
    instruction: Option<&str>,
) -> String {
    // ëª¨ë“  ì…ë ¥ sanitize
    let query_clean = sanitize_input(query);
    let docs_clean: Vec<String> = docs.iter().map(|d| sanitize_input(d)).collect();
    let k = docs.len();

    let mut prompt = String::with_capacity(
        1024 + query_clean.len() * 2 + docs_clean.iter().map(|d| d.len()).sum::<usize>()
    );

    // System ë©”ì‹œì§€ (TECHSPEC Â§7.1.1 ë° modeling.pyì™€ ì •í™•íˆ ì¼ì¹˜)
    // ê²½ê³ : ì´ ë¬¸ìì—´ì„ ìˆ˜ì •í•˜ë©´ ëª¨ë¸ í˜¸í™˜ì„±ì´ ê¹¨ì§‘ë‹ˆë‹¤ - ì¬í¬ë§·í•˜ì§€ ë§ˆì„¸ìš”
    prompt.push_str("<|im_start|>system\n");
    prompt.push_str("You are a search relevance expert who can determine a ranking of the passages based on how relevant they are to the query. If the query is a question, how relevant a passage is depends on how well it answers the question. If not, try to analyze the intent of the query and assess how well each passage satisfies the intent. If an instruction is provided, you should follow the instruction when determining the ranking.\n");
    prompt.push_str("<|im_end|>\n");

    // User ë©”ì‹œì§€ í—¤ë”
    prompt.push_str("<|im_start|>user\n");
    prompt.push_str(&format!(
        "I will provide you with {} passages, each indicated by a numerical identifier. \
         Rank the passages based on their relevance to query: {}\n",
        k, query_clean
    ));

    // ì„ íƒì  instruction ë¸”ë¡
    if let Some(instr) = instruction {
        prompt.push_str("<instruct>\n");
        prompt.push_str(instr);
        prompt.push_str("\n</instruct>\n");
    }

    // Passages
    for (i, doc) in docs_clean.iter().enumerate() {
        prompt.push_str(&format!("<passage id=\"{}\">\n", i));
        prompt.push_str(doc);
        prompt.push_str("<|embed_token|>\n</passage>\n");
    }

    // Query ë¸”ë¡ (ìƒŒë“œìœ„ì¹˜ íŒ¨í„´ - ì¿¼ë¦¬ê°€ ë‘ ë²ˆ ë‚˜íƒ€ë‚¨)
    prompt.push_str("<query>\n");
    prompt.push_str(&query_clean);
    prompt.push_str("<|rerank_token|>\n</query>\n");

    // Thinking placeholderê°€ ìˆëŠ” Assistant ë©”ì‹œì§€
    prompt.push_str("<|im_end|>\n");
    prompt.push_str("<|im_start|>assistant\n");
    prompt.push_str("<think>\n\n</think>\n\n");

    prompt
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_sanitize_removes_special_tokens() {
        let input = "Hello <|embed_token|> world <|rerank_token|> test";
        let result = sanitize_input(input);
        assert_eq!(result, "Hello  world  test");
    }

    #[test]
    fn test_build_prompt_structure() {
        let query = "What is Rust?";
        let docs = vec!["Rust is a systems programming language.", "Python is easy."];
        let prompt = build_jina_v3_prompt(query, &docs, None);

        // ì£¼ìš” ì»´í¬ë„ŒíŠ¸ í™•ì¸
        assert!(prompt.contains("<|im_start|>system"));
        assert!(prompt.contains("You are a search relevance expert"));
        assert!(prompt.contains("<|im_start|>user"));
        assert!(prompt.contains("I will provide you with 2 passages"));
        assert!(prompt.contains("<passage id=\"0\">"));
        assert!(prompt.contains("<passage id=\"1\">"));
        assert!(prompt.contains("<|embed_token|>"));
        assert!(prompt.contains("<|rerank_token|>"));
        assert!(prompt.contains("<query>"));
        assert!(prompt.contains("<|im_start|>assistant"));
        assert!(prompt.contains("<think>"));
    }

    #[test]
    fn test_build_prompt_with_instruction() {
        let query = "test query";
        let docs = vec!["doc1"];
        let prompt = build_jina_v3_prompt(query, &docs, Some("Focus on technical accuracy."));

        assert!(prompt.contains("<instruct>"));
        assert!(prompt.contains("Focus on technical accuracy."));
        assert!(prompt.contains("</instruct>"));
    }
}
```

### 2.2 í† í¬ë‚˜ì´ì œì´ì…˜ í™•ì¥

**íŒŒì¼:** `core/src/tokenization.rs`
**ìœ„ì¹˜:** ë‹¤ìŒ í•¨ìˆ˜ë“¤ ì¶”ê°€

```rust
use tokenizers::{Encoding, Tokenizer};
use anyhow::{anyhow, Result};

/// Listwise rerankingì„ ìœ„í•œ left paddingìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”©
///
/// Qwen3 ëª¨ë¸ì€ ì¸ê³¼ì„±ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ left paddingì´ í•„ìš”í•©ë‹ˆë‹¤.
///
/// âš ï¸ **SHOULD-FIX S2: í–¥ìƒëœ ë¬¸ì„œí™”**
/// - ì´ê²ƒì€ ë‹¨ì¼ ìƒ˜í”Œì„ ì¸ì½”ë”©í•©ë‹ˆë‹¤ (ë°°ì¹˜ ì—†ìŒ), ë”°ë¼ì„œ íŒ¨ë”©ì´ ì ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤
/// - Attention maskëŠ” íŒ¨ë“œ í† í°ì´ ì—†ìœ¼ë¯€ë¡œ ëª¨ë‘ 1ì…ë‹ˆë‹¤
/// - íŒ¨ë”©ì€ ì„œë¡œ ë‹¤ë¥¸ ê¸¸ì´ì˜ ì—¬ëŸ¬ ì‹œí€€ìŠ¤ë¥¼ ë°°ì¹˜í•  ë•Œë§Œ í•„ìš”í•©ë‹ˆë‹¤
/// - `add_special_tokens=true`ëŠ” HuggingFace Transformers ê¸°ë³¸ ë™ì‘ê³¼ ì¼ì¹˜í•©ë‹ˆë‹¤
///
/// # ì¸ì
/// * `tokenizer` - í† í¬ë‚˜ì´ì € ì¸ìŠ¤í„´ìŠ¤ (left paddingìœ¼ë¡œ ì„¤ì •ë˜ì–´ì•¼ í•¨)
/// * `prompt` - ì™„ì „í•œ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ (ì´ë¯¸ ëª¨ë“  íŠ¹ìˆ˜ í† í° í¬í•¨)
/// * `max_length` - ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (ì„ íƒì , ê²€ì¦ìš©)
///
/// # ë°˜í™˜
/// attention_mask=ëª¨ë‘ 1ì¸ í† í°í™”ëœ ì¸ì½”ë”© (ë‹¨ì¼ ìƒ˜í”Œì˜ ê²½ìš° íŒ¨ë”© ì—†ìŒ)
pub fn encode_listwise(
    tokenizer: &Tokenizer,
    prompt: &str,
    max_length: Option<usize>,
) -> Result<Encoding> {
    // ì¸ì½”ë”© ì •ì±… (S2): ë‹¨ì¼ ìƒ˜í”Œ (ë°°ì¹˜ ì—†ìŒ), íŒ¨ë”© ë¶ˆí•„ìš”
    // ë‹¨ì¼ ì‹œí€€ìŠ¤ ì¸ì½”ë”©ì—ëŠ” íŒ¨ë”©ì´ ì—†ìœ¼ë¯€ë¡œ ëª¨ë“  attention mask ê°’ì€ 1
    // íŒ¨ë”©ì€ ì—¬ëŸ¬ ì‹œí€€ìŠ¤ë¥¼ ë°°ì¹˜í•  ë•Œë§Œ ì ìš©ë¨

    // ì¤‘ìš”: add_special_tokens=trueëŠ” Python Transformers ê¸°ë³¸ê°’ê³¼ ì¼ì¹˜
    // ì •í™•í•œ ë¸”ë¡ ì²­í‚¹ì„ ìœ„í•´ modeling.pyì™€ í† í° ì¹´ìš´íŠ¸ê°€ ì¼ì¹˜í•˜ë„ë¡ ë³´ì¥
    // ChatML í† í°(<|im_start|>, <|im_end|>)ì„ ì¸ì½”ë”©ì— í¬í•¨
    let encoding = tokenizer
        .encode(prompt, true)  // falseì˜€ìŒ - í† í° ê¸¸ì´ ë¶ˆì¼ì¹˜ ë°œìƒ!
        .map_err(|e| anyhow!("Tokenization failed: {}", e))?;

    // ê¸¸ì´ ê²€ì¦
    if let Some(max_len) = max_length {
        if encoding.len() > max_len {
            return Err(anyhow!(
                "Prompt exceeds max length: {} > {}. Try reducing document count or length.",
                encoding.len(),
                max_len
            ));
        }
    }

    Ok(encoding)
}

// ì£¼ì˜: Padding side/tokenì€ ëª¨ë¸ ë¡œë“œ ì¤‘ì— ì„¤ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤ (Milestone 3.2 ì°¸ì¡°).

/// í† í° ì œí•œì„ ì ìš©í•˜ê¸° ìœ„í•´ í…ìŠ¤íŠ¸ ì ˆë‹¨ ë° ë””ì½”ë”©
///
/// Python ì°¸ì¡° `_truncate_texts` ë™ì‘ê³¼ ì¼ì¹˜:
/// - ì¿¼ë¦¬ëŠ” max_query_lengthë¡œ ì ˆë‹¨ (ê¸°ë³¸ê°’ 512)
/// - ê° ë¬¸ì„œëŠ” max_doc_lengthë¡œ ì ˆë‹¨ (ê¸°ë³¸ê°’ 2048)
/// - ë””ì½”ë”©ëœ ë¬¸ìì—´ê³¼ í† í° ê¸¸ì´ ë°˜í™˜
///
/// í† í¬ë‚˜ì´ì œì´ì…˜ ì •ì±…:
/// - HuggingFace Transformers ê¸°ë³¸ê°’ê³¼ ì¼ì¹˜í•˜ëŠ” `add_special_tokens=false` ì‚¬ìš©
/// - ì´ê²ƒì€ encode/decode ì‚¬ì´í´ì˜ í‘œì¤€ ë™ì‘
/// - íŠ¹ìˆ˜ í† í°(<|embed_token|>, <|rerank_token|>)ì€ í† í¬ë‚˜ì´ì €ê°€ ì•„ë‹Œ í”„ë¡¬í”„íŠ¸ ë¹Œë”ì— ì˜í•´ ì¶”ê°€ë¨
///
/// # ë°˜í™˜
/// (truncated_query, truncated_docs, doc_token_lengths, query_token_length)
pub fn truncate_texts(
    tokenizer: &Tokenizer,
    query: &str,
    documents: &[String],
    max_query_length: usize,
    max_doc_length: usize,
) -> Result<(String, Vec<String>, Vec<usize>, usize)> {
    // ì¤‘ìš” í† í¬ë‚˜ì´ì œì´ì…˜ ì •ì±… (modeling.py íŒ¨ë¦¬í‹°):
    // - encode(..., true): íŠ¹ìˆ˜ í† í° ì¶”ê°€ (HF Transformers ê¸°ë³¸ê°’ê³¼ ì¼ì¹˜)
    // - decode(..., true): ë””ì½”ë”©ì‹œ íŠ¹ìˆ˜ í† í° ê±´ë„ˆë›°ê¸° (í”„ë¡¬í”„íŠ¸ì— BOS/EOS ë°©ì§€)
    // ì™„ì „í•œ HF íŒ¨ë¦¬í‹°ë¥¼ ìœ„í•´ ë‘˜ ë‹¤ TRUEë¡œ ì„¤ì •

    // ì„±ëŠ¥: clone ë¶ˆí•„ìš” - ì´ í•¨ìˆ˜ ë™ì•ˆ í† í¬ë‚˜ì´ì €ëŠ” ë¶ˆë³€
    let tk = tokenizer;

    // ì¿¼ë¦¬
    let q_enc = tk.encode(query, true).map_err(|e| anyhow!("encode(query): {}", e))?;
    let mut query_ids = q_enc.get_ids().to_vec();
    let mut query_trunc = query.to_string();
    if query_ids.len() > max_query_length {
        query_ids.truncate(max_query_length);
        // skip_special_tokens=trueëŠ” HF decode ê¸°ë³¸ê°’ê³¼ ì¼ì¹˜
        query_trunc = tk.decode(&query_ids, true).map_err(|e| anyhow!("decode(query): {}", e))?;
    }
    let query_len = query_ids.len();

    // ë¬¸ì„œë“¤
    let mut docs_trunc = Vec::with_capacity(documents.len());
    let mut doc_lens   = Vec::with_capacity(documents.len());
    for d in documents {
        let d_enc = tk.encode(d, true).map_err(|e| anyhow!("encode(doc): {}", e))?;
        let mut ids = d_enc.get_ids().to_vec();
        if ids.len() > max_doc_length {
            ids.truncate(max_doc_length);
            // skip_special_tokens=trueëŠ” HF decode ê¸°ë³¸ê°’ê³¼ ì¼ì¹˜
            docs_trunc.push(tk.decode(&ids, true).map_err(|e| anyhow!("decode(doc): {}", e))?);
        } else {
            docs_trunc.push(d.clone());
        }
        doc_lens.push(ids.len());
    }

    Ok((query_trunc, docs_trunc, doc_lens, query_len))
}
```

---

## Milestone 3: ë°±ì—”ë“œ ì¶”ìƒí™”

### 3.1 Backend Trait í™•ì¥

**íŒŒì¼:** `backends/core/src/lib.rs`
**ìœ„ì¹˜:** ê¸°ì¡´ `Backend` trait ì •ì˜ ë’¤ì— ì¶”ê°€

```rust
use std::fmt;

/// ë‹¨ì¼ listwise ë¸”ë¡ì„ ìœ„í•´ ë°±ì—”ë“œë¡œ ì „ë‹¬ë˜ëŠ” ì…ë ¥ í˜ì´ë¡œë“œ.
#[derive(Debug, Clone)]
pub struct ListwiseBlockInput {
    pub input_ids: Vec<u32>,
    pub attention_mask: Vec<u32>,
    pub embed_token_id: u32,
    pub rerank_token_id: u32,
    pub doc_count: usize,
}

/// ë‹¨ì¼ listwise ë¸”ë¡ì— ëŒ€í•´ ë°±ì—”ë“œê°€ ë°˜í™˜í•˜ëŠ” ì¶œë ¥.
///
/// ì°¨ì› ì£¼ì˜: ì„ë² ë”©ì€ Jina Reranker v3 ì‚¬ì–‘ì— ë”°ë¼ 512ì°¨ì›ì…ë‹ˆë‹¤.
/// ì„¤ì • ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ê°€ ì•„ë‹™ë‹ˆë‹¤ - í›ˆë ¨ëœ projector ê°€ì¤‘ì¹˜ì— ì˜í•´ ê³ ì •ë©ë‹ˆë‹¤.
/// ë¯¸ë˜ ëª¨ë¸ ë²„ì „(ì˜ˆ: Jina v4)ì€ ë‹¤ë¥¸ ì°¨ì›ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ - ë‹¤ë¥¸ ê³³ì—ì„œ í•˜ë“œì½”ë”©í•˜ì§€ ë§ˆì„¸ìš”.
#[derive(Debug, Clone)]
pub struct ListwiseBlockOutput {
    pub query_embedding: Vec<f32>,     // 512-d (Jina v3 projector ì¶œë ¥ ì°¨ì›)
    pub doc_embeddings: Vec<Vec<f32>>, // ë¬¸ì„œë‹¹ 512-d (ê°™ì€ ì°¨ì›)
}

/// ê¸°ì¡´ ë°±ì—”ë“œ traitì„ opt-in listwise hookìœ¼ë¡œ í™•ì¥.
pub trait Backend {
    // ...ê¸°ì¡´ ë©”ì†Œë“œë“¤...

    fn embed_listwise_block(
        &self,
        _input: ListwiseBlockInput,
    ) -> Result<ListwiseBlockOutput, BackendError> {
        Err(BackendError::Unsupported(
            "listwise reranking not supported".into(),
        ))
    }
}
```

> Listwise rerankingì„ ì§€ì›í•˜ëŠ” ë°±ì—”ë“œ(ì˜ˆ: Candle Jina reranker)ëŠ” ë‹¨ìˆœíˆ ì´ ë©”ì†Œë“œë¥¼ ì˜¤ë²„ë¼ì´ë“œí•©ë‹ˆë‹¤. ê¸°ë³¸ êµ¬í˜„ì´ ì—ëŸ¬ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ê¸°ì¡´ì˜ pairwise ì „ìš© ë°±ì—”ë“œëŠ” ë³€ê²½ ì—†ì´ ê³„ì† ì»´íŒŒì¼ë˜ë©° ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤ ìŠ¤ë ˆë“œëŠ” ë‹¤ìš´ìºìŠ¤íŒ… ì—†ì´ `Box<dyn Backend>`ë¥¼ í†µí•´ ë””ìŠ¤íŒ¨ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> **ê°ì²´ ì•ˆì „ì„± ì£¼ì˜:** `Result<_, BackendError>`ê°€ ìˆëŠ” ê¸°ë³¸ êµ¬í˜„ì€ trait ê°ì²´ ì•ˆì „ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤. ë°±ì—”ë“œëŠ” ë‹¤ìš´ìºìŠ¤íŒ… ì—†ì´ `Box<dyn Backend>`ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì€ ë°±ì—”ë“œ íƒ€ì…ì´ ì§€ì›Œì§€ëŠ” ì›Œì»¤ ë””ìŠ¤íŒ¨ì¹˜ ì•„í‚¤í…ì²˜ì— ì¤‘ìš”í•©ë‹ˆë‹¤.

### 3.2 Listwise ëª¨ë¸ì„ ìœ„í•œ í† í¬ë‚˜ì´ì € ì„¤ì •

**íŒŒì¼:** `backends/candle/src/lib.rs` (ë˜ëŠ” ë°±ì—”ë“œ ì´ˆê¸°í™”ê°€ ë°œìƒí•˜ëŠ” ê³³)
**ìœ„ì¹˜:** `CandleBackend::new()` ì¤‘, í† í¬ë‚˜ì´ì € ë¡œë“œ í›„, ë°±ì—”ë“œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì „

**âš ï¸ ì¤‘ìš” ìœ„ì¹˜ ìš”êµ¬ì‚¬í•­:**
- ë°±ì—”ë“œ ì´ˆê¸°í™” ì¤‘ í•œ ë²ˆ í† í¬ë‚˜ì´ì € ì„¤ì • (ë‹¨ì¼ ìŠ¤ë ˆë“œ ì»¨í…ìŠ¤íŠ¸)
- ë¼ìš°í„° ìš”ì²­ í•¸ë“¤ëŸ¬ì—ì„œ ì„¤ì •í•˜ì§€ ë§ ê²ƒ - ê²½ìŸ ì¡°ê±´ ë°œìƒ!
- ì´ í•¨ìˆ˜ëŠ” `router/src/lib.rs`ê°€ ì•„ë‹Œ ë°±ì—”ë“œ ì´ˆê¸°í™” ì½”ë“œì—ì„œ í˜¸ì¶œë˜ì–´ì•¼ í•©ë‹ˆë‹¤

```rust
use anyhow::anyhow;
use tokenizers::{PaddingDirection, PaddingParams, PaddingStrategy, Tokenizer};

/// âš ï¸ ì¤‘ìš”: ë°±ì—”ë“œ ì´ˆê¸°í™”ì‹œ í•œ ë²ˆë§Œ í˜¸ì¶œ (ë‹¨ì¼ ìŠ¤ë ˆë“œ ì»¨í…ìŠ¤íŠ¸)
/// ë¼ìš°í„° ìš”ì²­ í•¸ë“¤ëŸ¬ì—ì„œ í˜¸ì¶œí•˜ì§€ ë§ ê²ƒ - ê²½ìŸ ì¡°ê±´ ìœ„í—˜!
///
/// ì´ í•¨ìˆ˜ëŠ” router/src/lib.rsê°€ ì•„ë‹Œ backends/candle/src/lib.rsì˜
/// ëª¨ë¸ ë¡œë”© ì¤‘ì— í˜¸ì¶œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
fn configure_lbnl_tokenizer(tokenizer: &mut Tokenizer) -> anyhow::Result<()> {
    use tokenizers::{PaddingDirection, PaddingParams, PaddingStrategy};

    // âš ï¸ ë¸”ë¡œì»¤ ìˆ˜ì •: í•­ìƒ ëª…ì‹œì ìœ¼ë¡œ padding ì„¤ì • (tokenizers ë²„ì „ í˜¸í™˜ì„±)
    // ì¼ë¶€ ë²„ì „ì€ get_padding()ì„ ì§€ì›í•˜ì§€ ì•Šê±°ë‚˜ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•˜ê²Œ None ë°˜í™˜

    // NIT 3: PAD í† í° ê²€ìƒ‰ ìˆœì„œ ë¬¸ì„œí™”
    // ìš°ì„ ìˆœìœ„: pad â†’ unk â†’ eos (TECHSPEC Â§6.4 + Jina v3 Python ì°¸ì¡°ì™€ ì¼ì¹˜)
    // ì´ í´ë°± ì‹œí€€ìŠ¤ëŠ” modeling.pyì˜ í† í¬ë‚˜ì´ì € ì„¤ì •ê³¼ ë™ì¼:
    // 1. ëª…ì‹œì  pad í† í° ë¨¼ì € ì‹œë„ (<|pad|>, <pad>, [PAD])
    // 2. unknown í† í°ìœ¼ë¡œ í´ë°± (<unk>, [UNK]) - Qwen3ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì´ê²ƒ ì‚¬ìš©
    // 3. EOSë¡œ ìµœì¢… í´ë°± (</s>, <|endoftext|>) - GPT ìŠ¤íƒ€ì¼ í† í¬ë‚˜ì´ì €ìš©
    const PAD_CANDIDATES: &[&str] = &[
        "<|pad|>", "<pad>", "[PAD]",       // ëª…ì‹œì  pad í† í°
        "<unk>", "[UNK]",                   // Unknown í† í° í´ë°± (Qwen3 ê¸°ë³¸ê°’)
        "</s>", "<|endoftext|>",           // GPT ìŠ¤íƒ€ì¼ìš© EOS í´ë°±
    ];

    let (pad_token, pad_id) = PAD_CANDIDATES
        .iter()
        .find_map(|t| tokenizer.token_to_id(t).map(|id| (t.to_string(), id)))
        .ok_or_else(|| anyhow!(
            "Tokenizer must have one of: {:?}. \
             Verify tokenizer_config.json includes pad_token, unk_token, or eos_token.",
            PAD_CANDIDATES
        ))?;

    tracing::info!(
        "Configuring LBNL tokenizer: pad_token='{}' (id={}), direction=Left, strategy=BatchLongest",
        pad_token, pad_id
    );

    // í•­ìƒ with_padding í˜¸ì¶œ (get_paddingì— ì˜ì¡´í•˜ì§€ ë§ ê²ƒ - ë²„ì „ í˜¸í™˜ì„±)
    tokenizer.with_padding(Some(PaddingParams {
        strategy: PaddingStrategy::BatchLongest,
        direction: PaddingDirection::Left,
        pad_id,
        pad_type_id: 0,
        pad_token,
    }));

    // ì„¤ì • ì„±ê³µ í™•ì¸
    if tokenizer.get_padding().is_none() {
        anyhow::bail!("Failed to configure tokenizer padding - check tokenizers crate version");
    }

    Ok(())
}

// ì¤‘ìš”: ìš”ì²­ë§ˆë‹¤ê°€ ì•„ë‹Œ ëª¨ë¸ ì´ˆê¸°í™”ì‹œ í•œ ë²ˆë§Œ í˜¸ì¶œ
// ì˜ˆì‹œ í†µí•© ì§€ì  (ë°±ì—”ë“œ ì´ˆê¸°í™” ì½”ë“œì—ì„œ):
// if matches!(model_kind, ModelKind::ListwiseReranker) {
//     configure_lbnl_tokenizer(&mut tokenizer)?;
// }
```

> **âš ï¸ ì„¤ì • ìœ„ì¹˜ ê²½ê³ :**
> ì´ í•¨ìˆ˜ëŠ” `backends/candle/src/lib.rs`ì˜ ë°±ì—”ë“œ ì´ˆê¸°í™” ì¤‘ì— í˜¸ì¶œë˜ì–´ì•¼ í•˜ë©°,
> ë¼ìš°í„°ì—ì„œ í˜¸ì¶œë˜ë©´ ì•ˆ ë©ë‹ˆë‹¤. ì—¬ëŸ¬ ìŠ¤ë ˆë“œì—ì„œ í† í¬ë‚˜ì´ì €ë¥¼ ì„¤ì •í•˜ë©´ ê²½ìŸ ì¡°ê±´ì´ ë°œìƒí•©ë‹ˆë‹¤.
> ë¼ìš°í„° í•¸ë“¤ëŸ¬ëŠ” ë©€í‹° ìŠ¤ë ˆë“œì´ë©° í† í¬ë‚˜ì´ì €ë¥¼ ì ˆëŒ€ ë³€ê²½í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.
> ê²½ìŸ ì¡°ê±´ì„ í”¼í•˜ê¸° ìœ„í•´ ë°±ì—”ë“œ ì´ˆê¸°í™” ì¤‘ í•œ ë²ˆë§Œ ì„¤ì •í•˜ì„¸ìš”.

> **ì´ìœ :** Qwen3 ê¸°ë°˜ rerankerëŠ” `<|embed_token|>`/`<|rerank_token|>` hidden-state ìœ„ì¹˜ê°€ ì •ë ¬ë˜ë„ë¡ left paddingì— ì˜ì¡´í•©ë‹ˆë‹¤.

### 3.3 ë°±ì—”ë“œ ì»¤ë§¨ë“œ ë””ìŠ¤íŒ¨ì¹˜

**íŒŒì¼:** `backends/src/lib.rs`
**ìœ„ì¹˜:**
1. `BackendCommand` enum í™•ì¥
2. `BackendThread::new` match arm ì—…ë°ì´íŠ¸

**ì¤‘ìš”:** ë¹„ë™ê¸° `embed_listwise_block()` ë©”ì†Œë“œëŠ” `Backend`ê°€ ì•„ë‹Œ `Infer`ì— êµ¬í˜„ë©ë‹ˆë‹¤ (Milestone 9.3 ì°¸ì¡°).
ì´ë ‡ê²Œ í•˜ë©´ ì¤‘ë³µì„ í”¼í•˜ê³  ì±„ë„ ë””ìŠ¤íŒ¨ì¹˜ ë¡œì§ì„ ì¤‘ì•™í™”í•©ë‹ˆë‹¤.

```rust
use text_embeddings_backend_core::{ListwiseBlockInput, ListwiseBlockOutput};

// ì£¼ì˜: ì—¬ê¸°ì— `impl Backend { async fn ... }` ì—†ìŒ - ì¤‘ë³µ ë°œìƒ!
// ë¹„ë™ê¸° ë˜í¼ëŠ” `Infer::embed_listwise_block()`ì— ìˆìŠµë‹ˆë‹¤ (Milestone 9.3)

enum BackendCommand {
    // ... ê¸°ì¡´ variantë“¤ ...
    EmbedListwise(
        ListwiseBlockInput,
        Span,
        oneshot::Sender<Result<ListwiseBlockOutput, BackendError>>,
    ),
}

impl BackendThread {
    fn new(
        backend: Box<dyn CoreBackend + Send>,
        mut backend_receiver: mpsc::Receiver<BackendCommand>,
        health_sender: watch::Sender<bool>,
    ) -> Self {
        let handle = std::thread::spawn(move || {
            while let Some(cmd) = backend_receiver.blocking_recv() {
                let start = Instant::now();
                let mut healthy = false;
                match cmd {
                    // ... ê¸°ì¡´ armë“¤ ...
                    BackendCommand::EmbedListwise(input, span, sender) => {
                        let _span = span.entered();
                        let result = backend.embed_listwise_block(input).map(|out| {
                            healthy = true;
                            (out, start.elapsed())
                        });
                        let _ = sender.send(result.map(|(out, _)| out));
                    }
                }
                let _ = health_sender.send(healthy);
            }
        });
        Self(Some(handle))
    }
}
```

---

## Milestone 4: Candle ë°±ì—”ë“œ êµ¬í˜„

### 4.0 Qwen3 Hidden-State API

**íŒŒì¼:** `backends/candle/src/models/qwen3.rs`
**ìœ„ì¹˜:** `impl Qwen3Model` ë‚´ë¶€

```rust
use candle::{Result, Tensor};
use text_embeddings_backend_core::Batch;

impl Qwen3Model {
    /// ì „ì²´ ìˆœë°©í–¥ ì „ë‹¬ì„ ì‹¤í–‰í•˜ê³  ìµœì¢… hidden states ë°˜í™˜ (RMSNorm í›„)
    /// pooling/projection ë¡œì§ì„ ì ìš©í•˜ì§€ ì•ŠìŒ.
    ///
    /// âš ï¸ **ë¸”ë¡œì»¤ B1 - ì™„ì „í•œ êµ¬í˜„ ì œê³µ**
    ///
    /// ì´ê²ƒì€ ì™„ì „í•˜ê³  ì»´íŒŒì¼ ê°€ëŠ¥í•œ êµ¬í˜„ì…ë‹ˆë‹¤. í•µì‹¬ì€ ê³µìœ  ë¡œì§ì„
    /// `forward_layers()`ë¡œ ì¶”ì¶œí•˜ì—¬ `embed()`ì™€ `forward_hidden_states()` ê°„
    /// ì½”ë“œ ì¤‘ë³µì„ í”¼í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
    ///
    /// ì¤‘ìš” ìš”êµ¬ì‚¬í•­:
    /// 1. ê¸°ì¡´ ë ˆì´ì–´ ë£¨í”„ ë¡œì§ ì¬ì‚¬ìš© (ì½”ë“œ ì¤‘ë³µ ì—†ìŒ)
    /// 2. embed()ì™€ ë™ì¼í•œ mask/RoPE/attention-bias ì²˜ë¦¬
    /// 3. ìµœì¢… RMSNorm í›„ hidden states ë°˜í™˜ (PyTorch `hidden_states[-1]`ì™€ ì¼ì¹˜)
    /// 4. ëª¨ë¸ì˜ ë„¤ì´í‹°ë¸Œ dtype ìœ ì§€ (BF16/FP16/F32)

    /// ê³µìœ  ìˆœë°©í–¥ ì „ë‹¬ ë¡œì§ - ê¸°ì¡´ embed()ì—ì„œ ì¶”ì¶œ
    ///
    /// ì´ ë©”ì†Œë“œëŠ” embed()ì™€ forward_hidden_states() ëª¨ë‘ í•„ìš”í•œ
    /// í•µì‹¬ ë ˆì´ì–´ë³„ ì²˜ë¦¬ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ì¶”ì¶œí•¨ìœ¼ë¡œì¨ ëª¨ë¸ êµ¬í˜„ì´
    /// ë³€ê²½ë  ë•Œ ë™ê¸°í™” ìƒíƒœë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
    ///
    /// ë°˜í™˜: ìµœì¢… RMSNorm í›„ Hidden states, ëª¨ë¸ì˜ ë„¤ì´í‹°ë¸Œ dtype
    fn forward_layers(&self, input_ids: &Tensor, attention_mask: &Tensor) -> Result<Tensor> {
        // ë‹¨ê³„ 1: ì…ë ¥ í† í° ì„ë² ë“œ
        let mut hidden = self.embed_tokens.forward(input_ids)?;

        // ë‹¨ê³„ 2: RoPE ì„ë² ë”© ì¤€ë¹„
        // ì¤‘ìš”: embed()ì™€ ê°™ì€ ì‹œí€€ìŠ¤ ê¸¸ì´ ê³„ì‚° ì‚¬ìš©
        let seq_len = input_ids.dim(1)?;
        let (cos, sin) = self.rotary_emb.forward(seq_len)?;

        // ë‹¨ê³„ 3: Attention mask/bias ì¤€ë¹„
        // ì¤‘ìš”: embed() êµ¬í˜„ì˜ ì •í™•í•œ dtype ë° shape ì¼ì¹˜
        // TEIì˜ Qwen3ê°€ attention_bias ë˜ëŠ” raw mask ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸
        let attention_bias = if self.use_attention_bias {
            // bias ì‚¬ìš©ì‹œ maskë¥¼ bias í…ì„œë¡œ ë³€í™˜
            // ì´ê²ƒì€ ê¸°ì¡´ embed() ê²½ë¡œì™€ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•¨
            Some(self.prepare_attention_bias(attention_mask)?)
        } else {
            // raw mask ì‚¬ìš©ì‹œ ì˜¬ë°”ë¥¸ dtype ë³´ì¥ (ì¼ë°˜ì ìœ¼ë¡œ I64 ë˜ëŠ” U32)
            // attention_maskëŠ” ì´ë¯¸ Batchì˜ ì˜¬ë°”ë¥¸ í˜•ì‹
            None
        };

        // ë‹¨ê³„ 4: ë ˆì´ì–´ë³„ ìˆœë°©í–¥ ì „ë‹¬
        // ì¤‘ìš”: ì´ ë£¨í”„ëŠ” ê¸°ì¡´ embed()ì™€ ë™ì¼í•´ì•¼ í•¨
        for layer in &self.layers {
            hidden = layer.forward(&hidden, &cos, &sin, attention_bias.as_ref())?;
        }

        // ë‹¨ê³„ 5: ìµœì¢… RMSNorm
        // ì¤‘ìš”: ì¶œë ¥ì´ PyTorchì˜ hidden_states[-1]ê³¼ ì¼ì¹˜í•˜ë„ë¡ í•¨
        let hidden = self.norm.forward(&hidden)?;

        // ë„¤ì´í‹°ë¸Œ dtypeìœ¼ë¡œ ë°˜í™˜ (ëª¨ë¸ì´ ë¡œë“œëœ BF16/FP16/F32)
        Ok(hidden)  // Shape: [batch_size, seq_len, hidden_size]
    }

    /// LBNL projectorë¥¼ ìœ„í•œ ìµœì¢… hidden states ì¶”ì¶œ
    ///
    /// ì´ê²ƒì€ listwise rerankingì„ ìœ„í•œ public ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.
    /// ì „ì²´ ìˆœë°©í–¥ ì „ë‹¬ì„ ì‹¤í–‰í•˜ê³  ìµœì¢… RMSNorm í›„ hidden statesë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ///
    /// PYTORCH ëŒ€ë¹„ ê²€ì¦:
    /// - ìˆ˜ì¹˜ íŒ¨ë¦¬í‹°: rtol=1e-5, atol=1e-6
    /// - ì¼ì¹˜í•´ì•¼ í•¨: model(input_ids, attention_mask).hidden_states[-1]
    pub fn forward_hidden_states(&self, batch: Batch) -> Result<Tensor> {
        self.forward_layers(&batch.input_ids, &batch.attention_mask)
    }

    /// ì›ì‹œ í…ì„œë¥¼ ë°›ëŠ” í¸ì˜ í—¬í¼ (Python ì‹œê·¸ë‹ˆì²˜ì™€ ì¼ì¹˜)
    ///
    /// í† í°í™”ëœ í”„ë¡¬í”„íŠ¸ì—ì„œ ì§ì ‘ í…ì„œë¥¼ êµ¬ì„±í•˜ëŠ” LBNL ë°±ì—”ë“œì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.
    /// ë‚´ë¶€ì ìœ¼ë¡œ ê¸°ì¡´ ë°°ì¹˜ ì¸í”„ë¼ë¥¼ ì¬ì‚¬ìš©í•˜ê¸° ìœ„í•´ Batch structë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    pub fn forward_with_hidden_states(
        &self,
        input_ids: &Tensor,
        attention_mask: &Tensor,
    ) -> Result<Tensor> {
        // ì¤‘ìš”: attention_mask dtypeì´ forward_layers()ê°€ ì˜ˆìƒí•˜ëŠ” ê²ƒê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
        // Qwen3ê°€ I64ë¥¼ ì˜ˆìƒí•˜ë©´ ì—¬ê¸°ì„œ ë³€í™˜:
        // let attention_mask = attention_mask.to_dtype(DType::I64)?;

        // ì›ì‹œ í…ì„œì—ì„œ Batch êµ¬ì„±
        // ì£¼ì˜: ì‹¤ì œ TEI ì½”ë“œì— ëŒ€í•´ Batch ìƒì„±ì ì‹œê·¸ë‹ˆì²˜ í™•ì¸
        // êµ¬í˜„ì— ë”°ë¼ Batch::new() ë˜ëŠ” Batch::from_tensors()ì¼ ìˆ˜ ìˆìŒ
        let batch = Batch::from_padded(input_ids.clone(), attention_mask.clone())?;
        self.forward_hidden_states(batch)
    }

    /// ê¸°ì¡´ embed() ë©”ì†Œë“œ - ê³µìœ  forward_layers() ì‚¬ìš©í•˜ë„ë¡ ë¦¬íŒ©í† ë§
    ///
    /// âš ï¸ ë¦¬íŒ©í† ë§ í•„ìš”:
    /// ê¸°ì¡´ embed() êµ¬í˜„ì€ ë ˆì´ì–´ ë£¨í”„ë¥¼ ì¤‘ë³µí•˜ëŠ” ëŒ€ì‹  forward_layers()ë¥¼
    /// í˜¸ì¶œí•˜ë„ë¡ ìˆ˜ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆì‹œ:
    ///
    /// ```rust
    /// pub fn embed(&self, batch: Batch) -> Result<Embeddings> {
    ///     // ê³µìœ  ë ˆì´ì–´ ì²˜ë¦¬ ì‚¬ìš©
    ///     let hidden = self.forward_layers(&batch.input_ids, &batch.attention_mask)?;
    ///
    ///     // pooling(mean/cls ë“±) ë° ìµœì¢… projection ì ìš©
    ///     // ì´ ë¶€ë¶„ì€ ì›ë˜ embed()ì—ì„œ ë³€ê²½ë˜ì§€ ì•ŠìŒ
    ///     let pooled = self.pool(&hidden, &batch)?;
    ///     let embeddings = self.projection.forward(&pooled)?;
    ///
    ///     Ok(Embeddings {
    ///         values: embeddings,
    ///         // ... ë‹¤ë¥¸ í•„ë“œë“¤ ...
    ///     })
    /// }
    /// ```
}
```

**B1 ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- âœ… ê³µìœ  `forward_layers()` ë©”ì†Œë“œê°€ ì½”ë“œ ì¤‘ë³µ ì œê±°
- âœ… ì›ë˜ `embed()`ì™€ ë™ì¼í•œ RoPE/mask/bias ì²˜ë¦¬
- âœ… ìµœì¢… `norm.forward()` í›„ hidden states ë°˜í™˜ (PyTorch `hidden_states[-1]`ê³¼ ì¼ì¹˜)
- âœ… ëª¨ë¸ì˜ ë„¤ì´í‹°ë¸Œ dtype ìœ ì§€ (ê°•ì œ F32 ë³€í™˜ ì—†ìŒ)
- âš ï¸ **TODO:** Python ì°¸ì¡°ì™€ ìˆ˜ì¹˜ íŒ¨ë¦¬í‹° í…ŒìŠ¤íŠ¸ (rtol=1e-5, atol=1e-6)
- âš ï¸ **TODO:** `attention_mask` dtype (I64/U32/Bool)ì´ TEIì˜ Qwen3 ì˜ˆìƒê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
- âš ï¸ **TODO:** `Batch::from_padded()`ê°€ ì˜¬ë°”ë¥¸ TEI APIì¸ì§€ í™•ì¸ (`Batch::new()`ì¼ ìˆ˜ ìˆìŒ)

> **êµ¬í˜„ ì£¼ì˜:** ê¸°ì¡´ `embed()` ë©”ì†Œë“œë¥¼ ë¦¬íŒ©í† ë§í•  ë•Œ í˜„ì¬ ë ˆì´ì–´ ë£¨í”„ë¥¼ `forward_layers()`ë¡œ ì¶”ì¶œí•˜ì„¸ìš”. ê·¸ëŸ¬ë©´ ë‘ ë©”ì†Œë“œ ëª¨ë‘ ì´ ê³µìœ  êµ¬í˜„ì„ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ ì½”ë“œê°€ ë³€ê²½ë  ë•Œ ë™ê¸°í™” ìƒíƒœë¥¼ ìœ ì§€í•©ë‹ˆë‹¤. `embed()` ë©”ì†Œë“œëŠ” ì›ì‹œ hidden states ìœ„ì— poolingê³¼ projectionì„ ì¶”ê°€í•©ë‹ˆë‹¤.

### 4.1 Projector ë ˆì´ì–´

**íŒŒì¼:** `backends/candle/src/layers/projector.rs` (ì‹ ê·œ)

```rust
//! Jina v3 Rerankerë¥¼ ìœ„í•œ MLP Projector
//!
//! ì•„í‚¤í…ì²˜: Linear(hidden_size â†’ hidden_size/2, bias=False) â†’ ReLU â†’ Linear(hidden_size/2 â†’ 512, bias=False)

use candle_core::{Result, Tensor};
use candle_nn::{Linear, VarBuilder};

#[derive(Debug)]
pub struct Projector {
    fc1: Linear,
    fc2: Linear,
}

impl Projector {
    /// VarBuilderì—ì„œ projector ê°€ì¤‘ì¹˜ ë¡œë“œ
    ///
    /// âš ï¸ **SHOULD-FIX 4: DTYPE ê°•ì œ**
    /// ì¤‘ìš”: í˜¸ì¶œ ì§€ì ì—ì„œ ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ê¸° ì „ì— `vb.set_dtype(model_dtype)` ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤!
    /// ì˜ˆì‹œ: `Projector::load(vb.set_dtype(qwen3_dtype), hidden_size)?`
    ///
    /// ëŒ€ì•ˆ ì ‘ê·¼ë²• (ë” ëª…ì‹œì ):
    /// dtype íŒŒë¼ë¯¸í„° ì¶”ê°€: `pub fn load(vb: VarBuilder, hidden_size: usize, dtype: DType)`
    /// ê·¸ëŸ° ë‹¤ìŒ ì‚¬ìš©: `let vb = vb.set_dtype(dtype);` ì²« ë²ˆì§¸ ì¤„ë¡œ
    pub fn load(vb: VarBuilder, hidden_size: usize) -> Result<Self> {
        // SHOULD-FIX 4: ë°©ì–´ì  dtype ê²€ì¦
        // vb.dtype()ì— ì ‘ê·¼ ê°€ëŠ¥í•˜ë©´ ì—¬ê¸°ì„œ ì˜ˆìƒ ëª¨ë¸ dtypeê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
        // ì˜ˆì‹œ: assert_eq!(vb.dtype(), expected_dtype, "Projector dtype mismatch");

        let latent_size = hidden_size / 2; // modeling.py: hidden_size â†’ hidden_size/2 â†’ 512

        // VarBuilder ê²½ë¡œëŠ” safetensors í‚¤ì— ë§¤í•‘:
        // vb.pp("projector").pp("0") â†’ "projector.0.weight"
        // vb.pp("projector").pp("2") â†’ "projector.2.weight"
        let w1 = vb.pp("projector").pp("0").get((latent_size, hidden_size), "weight")?;
        let w2 = vb.pp("projector").pp("2").get((512, latent_size), "weight")?;

        // ì¤‘ìš”: Projectorì— biasê°€ ì—†ëŠ”ì§€ ê²€ì¦ (modeling.py: bias=False)
        // ì£¼ì˜: ì¡´ì¬ í™•ì¸ì„ ìœ„í•´ .get().is_ok() ì‚¬ìš© (ë¡œë“œ ì‹œë„í•˜ì§€ë§Œ ìµœì†Œí•œì˜ ì˜¤ë²„í—¤ë“œ)
        // Bias ì¡´ì¬ëŠ” í˜¸í™˜ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ì„ ë‚˜íƒ€ëƒ„ - ì¡°ê¸°ì— ê±°ë¶€í•˜ì—¬ ì¡°ìš©í•œ ì—ëŸ¬ ë°©ì§€
        if vb.pp("projector").pp("0").get::<Tensor, _>((latent_size,), "bias").is_ok()
            || vb.pp("projector").pp("2").get::<Tensor, _>((512,), "bias").is_ok()
        {
            candle_core::bail!(
                "Projector must be bias-free (bias=False per Jina v3 spec). \
                 This model may not be compatible. Verify weights or use --reranker-mode pairwise"
            );
        }

        let fc1 = Linear::new(w1, None);
        let fc2 = Linear::new(w2, None);
        Ok(Self { fc1, fc2 })
    }

    pub fn forward(&self, hidden: &Tensor) -> Result<Tensor> {
        let x = self.fc1.forward(hidden)?.relu()?;
        self.fc2.forward(&x)
    }
}
```

### 4.2 LBNL Reranker (Qwen3 + Projector)

**íŒŒì¼:** `backends/candle/src/models/lbnl_reranker.rs` (ì‹ ê·œ)

```rust
//! LBNL Reranker ëª¨ë¸: Qwen3 + MLP Projector
use candle_core::{Device, Result as CResult, Tensor};
use candle_nn::VarBuilder;
use crate::layers::projector::Projector;
use crate::models::qwen3::Qwen3Model;
use text_embeddings_backend_core::{Backend, BackendError, Batch, ListwiseBlockInput, ListwiseBlockOutput};

pub struct LbnlReranker {
    qwen3: Qwen3Model,
    projector: Projector,
    device: Device,
    dtype: candle_core::DType,  // ì¤‘ìš”: ëª¨ë¸ì˜ ë„¤ì´í‹°ë¸Œ dtype ì¶”ì  (BF16/FP16/F32)
}

impl LbnlReranker {
    pub fn new(
        vb: VarBuilder,
        qwen3: Qwen3Model,
        device: Device,
        hidden_size: usize,
        dtype: candle_core::DType,  // ëª¨ë¸ì˜ ë¡œë“œëœ dtype ì „ë‹¬
    ) -> CResult<Self> {
        // ì¤‘ìš”: Qwen3 ëª¨ë¸ê³¼ ê°™ì€ dtypeìœ¼ë¡œ projector ë¡œë“œ
        // ìˆœë°©í–¥ ì „ë‹¬ ì¤‘ mixed-precision ì´ìŠˆ ë°©ì§€
        let projector = Projector::load(vb.set_dtype(dtype), hidden_size)?;
        Ok(Self { qwen3, projector, device, dtype })
    }

    pub fn forward(&self, input: &ListwiseBlockInput) -> anyhow::Result<ListwiseBlockOutput> {
        let t = input.input_ids.len();
        let ids = Tensor::from_vec(input.input_ids.clone(), (1, t), &self.device)?;

        // âš ï¸ **í•„ìˆ˜ ìˆ˜ì • 3: DTYPE/SHAPE ì•ˆì „ì„±ì„ ìœ„í•´ BATCH ê²½ë¡œ ì‚¬ìš©**
        //
        // ì¤‘ìš” ìˆ˜ì •: ì›ì‹œ í…ì„œë¡œ forward_with_hidden_states()ë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ëŠ” ëŒ€ì‹ 
        // embed()ê°€ ì‚¬ìš©í•˜ëŠ” ë™ì¼í•œ Batch êµ¬ì„± ê²½ë¡œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ê²ƒì€ ë³´ì¥í•©ë‹ˆë‹¤:
        // 1. ì˜¬ë°”ë¥¸ attention_mask dtype (I64/U32/Bool - embed()ê°€ ì˜ˆìƒí•˜ëŠ” ê²ƒ)
        // 2. ì˜¬ë°”ë¥¸ shape ë° ë³€í™˜ (bias ë³€í™˜ ë“±)
        // 3. Qwen3 êµ¬í˜„ ë³€ê²½ì— ëŒ€í•´ ë¯¸ë˜ ë³´ì¥
        //
        // ì•ˆì „í•œ ì ‘ê·¼ë²• - Batch êµ¬ì„± ì‚¬ìš© (embed() ê²½ë¡œì™€ ì •í™•íˆ ì¼ì¹˜):
        let mask = Tensor::from_vec(
            input.attention_mask.clone(),
            (1, t),
            &self.device
        )?;  // ì´ˆê¸° í…ì„œ ìƒì„±

        // í•„ìˆ˜ ìˆ˜ì • 3: embed()ì™€ dtype/shape ì¼ê´€ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ Batch ìƒì„±
        // ê°€ì¥ ì•ˆì „í•œ ì ‘ê·¼ë²• - embed()ì™€ ì •í™•íˆ ê°™ì€ ê²½ë¡œ ì¬ì‚¬ìš©
        let batch = text_embeddings_backend_core::Batch::from_padded(
            ids.clone(),
            mask.clone()
        )?;

        // ì´ì œ forward_hidden_states()ëŠ” embed()ê°€ ë‚´ë¶€ì ìœ¼ë¡œ í•˜ëŠ” ê²ƒê³¼ ì •í™•íˆ ê°™ì´ mask ì²˜ë¦¬
        // - ê°™ì€ dtype ë³€í™˜
        // - ê°™ì€ bias ê³„ì‚°
        // - ê°™ì€ attention mask ì²˜ë¦¬
        let hs = self.qwen3.forward_hidden_states(batch)?;

        // Hidden statesê°€ ì˜ˆìƒ dtypeì¸ì§€ í™•ì¸ (ì´ë¯¸ ê·¸ë˜ì•¼ í•˜ì§€ë§Œ ê²€ì¦)
        let hs = if hs.dtype() != self.dtype {
            tracing::warn!("Hidden states dtype mismatch: got {:?}, expected {:?}", hs.dtype(), self.dtype);
            hs.to_dtype(self.dtype)?
        } else {
            hs
        };

        // íŠ¹ìˆ˜ í† í° ìœ„ì¹˜ ì°¾ê¸°
        let mut doc_pos = Vec::with_capacity(input.doc_count);
        let mut rerank_pos = None;
        for (i, &tid) in input.input_ids.iter().enumerate() {
            if tid == input.embed_token_id { doc_pos.push(i); }
            if tid == input.rerank_token_id { rerank_pos = Some(i); }
        }
        let qpos = rerank_pos.ok_or_else(|| anyhow::anyhow!("No rerank token found"))?;

        // ìœ„ì¹˜ì—ì„œ hidden states ì¶”ì¶œ â†’ ë„¤ì´í‹°ë¸Œ dtypeì˜ [1, H]
        let hq = hs.i((0, qpos, ..))?.unsqueeze(0)?;

        // ë¬¸ì„œ ì²˜ë¦¬: ë„¤ì´í‹°ë¸Œ dtypeì˜ projector, ë²¡í„° ì¶”ì¶œì‹œì—ë§Œ F32ë¡œ ë³€í™˜
        let mut doc_embs = Vec::with_capacity(doc_pos.len());
        for &p in &doc_pos {
            let hd = hs.i((0, p, ..))?.unsqueeze(0)?;
            // ProjectorëŠ” ë„¤ì´í‹°ë¸Œ dtypeìœ¼ë¡œ ì‘ë™ (BF16/FP16) - ë” ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
            let zd_native = self.projector.forward(&hd)?;
            // Vec<f32>ë¡œ ì¶”ì¶œí•  ë•Œë§Œ F32ë¡œ ë³€í™˜
            let zd_f32 = zd_native.to_dtype(candle_core::DType::F32)?;
            doc_embs.push(zd_f32.to_vec2::<f32>()?.remove(0));
        }

        // ì¿¼ë¦¬ ì²˜ë¦¬: ê°™ì€ dtype ì •ì±…
        let zq_native = self.projector.forward(&hq)?;
        let zq_f32 = zq_native.to_dtype(candle_core::DType::F32)?;
        let zq_vec = zq_f32.to_vec2::<f32>()?.remove(0);

        // ì¤‘ìš” ì •ê·œí™” ì •ì±… (modeling.py íŒ¨ë¦¬í‹°):
        // - Projector ì¶œë ¥ì€ L2 ì •ê·œí™” ì—†ì´ ë°˜í™˜ë¨
        // - ë¼ìš°í„° í•¸ë“¤ëŸ¬ê°€ cosine_similarity() ë‚´ì—ì„œ ì •ê·œí™” ìˆ˜í–‰
        // - ì´ê²ƒì€ normalize()ê°€ compute_scores() ë‚´ì—ì„œ í˜¸ì¶œë˜ëŠ” Python ì°¸ì¡°ì™€ ì¼ì¹˜
        // - ì—¬ê¸°ì„œ ì •ê·œí™”í•˜ë©´ ì´ì¤‘ ì •ê·œí™” ë°œìƒ!

        Ok(ListwiseBlockOutput { query_embedding: zq_vec, doc_embeddings: doc_embs })
    }
}

// ì¤‘ìš”: Backend trait êµ¬í˜„ (ë³„ë„ì˜ ListwiseBackend ì•„ë‹˜)
// ë‹¤ìš´ìºìŠ¤íŒ… ì—†ì´ Box<dyn Backend>ë¥¼ í†µí•œ ë””ìŠ¤íŒ¨ì¹˜ í—ˆìš©
impl Backend for LbnlReranker {
    fn health(&self) -> Result<(), BackendError> {
        Ok(())  // ëª¨ë¸ ë¡œë“œ ì„±ê³µ
    }

    fn is_padded(&self) -> bool {
        true  // Qwen3ëŠ” left padding ì‚¬ìš©
    }

    fn embed(&self, _batch: Batch) -> Result<text_embeddings_backend_core::Embeddings, BackendError> {
        Err(BackendError::Inference(
            "LBNL reranker only supports embed_listwise_block, not standard embedding".into()
        ))
    }

    fn predict(&self, _batch: Batch) -> Result<text_embeddings_backend_core::Predictions, BackendError> {
        Err(BackendError::Inference(
            "LBNL reranker only supports embed_listwise_block, not pairwise prediction".into()
        ))
    }

    // Listwise ì§€ì›ì„ ì œê³µí•˜ê¸° ìœ„í•´ ê¸°ë³¸ êµ¬í˜„ ì˜¤ë²„ë¼ì´ë“œ
    fn embed_listwise_block(&self, input: ListwiseBlockInput)
        -> Result<ListwiseBlockOutput, BackendError>
    {
        self.forward(&input).map_err(|e| BackendError::Inference(e.to_string()))
    }
}
```

### 4.3 CandleBackend::new í†µí•©

**íŒŒì¼:** `backends/candle/src/lib.rs`
**ìœ„ì¹˜:** `CandleBackend::new` ë‚´ë¶€, ë©”ì¸ `match config { ... }` ì „

```rust
use crate::models::{lbnl_reranker::LbnlReranker, Qwen3Model};

if let Config::Qwen3(qwen3_cfg) = &config {
    if has_lbnl_signature(&model_path, &tokenizer)? {
        tracing::info!("Detected LBNL reranker; loading Candle integration");

        let qwen3_model = Qwen3Model::load(vb.pp("model"), qwen3_cfg, model_type.clone())?;

        // ì¤‘ìš”: ëª¨ë¸ì˜ ë„¤ì´í‹°ë¸Œ dtype ê°€ì ¸ì˜¤ê¸° (BF16/FP16/F32) mixed-precision ë²„ê·¸ ë°©ì§€
        let dtype = qwen3_model.dtype(); // ë˜ëŠ” qwen3_modelì´ ë…¸ì¶œí•˜ì§€ ì•Šìœ¼ë©´ vb.dtype()

        let projector_vb = vb.pp("projector"); // ê°€ì¤‘ì¹˜ê°€ flat ë˜ëŠ” ë‹¤ë¥´ê²Œ ìƒ¤ë”©ë˜ë©´ ì¡°ì •
        let lbnl = LbnlReranker::new(
            projector_vb,
            qwen3_model,
            device.clone(),
            qwen3_cfg.hidden_size,
            dtype,  // ì¤‘ìš”: projectorê°€ ëª¨ë¸ê³¼ ê°™ì€ dtype ì‚¬ìš©í•˜ë„ë¡ dtype ì „ë‹¬
        )?;

        return Ok(Self {
            device,
            model: Box::new(lbnl),
            dense: None,
        });
    }
}
```

### 4.4 ëª¨ë“ˆ ì„ ì–¸

ì»´íŒŒì¼ëŸ¬ê°€ ìƒˆ ëª¨ë“ˆì„ ë³¼ ìˆ˜ ìˆë„ë¡ ì´ export ì¶”ê°€:

```rust
// backends/candle/src/models/mod.rs
pub mod lbnl_reranker;
pub use lbnl_reranker::LbnlReranker;

// backends/candle/src/layers/mod.rs
pub mod projector;
pub use projector::Projector;

// router/src/lib.rs
pub mod listwise;
pub mod strategy;

// core/src/lib.rs
pub mod prompt;
```

---

## Milestone 5: ë¼ìš°í„° í†µí•© - íŠ¹ìˆ˜ í† í° ê²€ì¦

### 5.1 íŠ¹ìˆ˜ í† í° ê²€ì¦

**íŒŒì¼:** `core/src/tokenization.rs`
**ìœ„ì¹˜:** `truncate_texts` í•¨ìˆ˜ ë’¤ì— ì¶”ê°€

```rust
/// í† í°í™”ëœ í”„ë¡¬í”„íŠ¸ê°€ ì˜ˆìƒë˜ëŠ” íŠ¹ìˆ˜ í† í° ê°œìˆ˜ë¥¼ í¬í•¨í•˜ëŠ”ì§€ ê²€ì¦
///
/// Hidden statesì—ì„œ ì„ë² ë”© ì¶”ì¶œì‹œ ë²”ìœ„ ë°– ì ‘ê·¼ ë°©ì§€.
///
/// # ì¸ì
/// * `input_ids` - í† í°í™”ëœ ì‹œí€€ìŠ¤
/// * `embed_token_id` - `<|embed_token|>`ì˜ ID
/// * `rerank_token_id` - `<|rerank_token|>`ì˜ ID
/// * `expected_doc_count` - í”„ë¡¬í”„íŠ¸ì˜ ë¬¸ì„œ ê°œìˆ˜
///
/// # ì—ëŸ¬
/// ë‹¤ìŒ ê²½ìš° ì—ëŸ¬ ë°˜í™˜:
/// - embed í† í° ê°œìˆ˜ê°€ ë¬¸ì„œ ê°œìˆ˜ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠìŒ
/// - rerank í† í° ê°œìˆ˜ê°€ ì •í™•íˆ 1ì´ ì•„ë‹˜
///
/// # ì˜ˆì‹œ
/// ```rust
/// let input_ids = vec![100, 151670, 200, 151670, 300, 151671, 400];
/// validate_special_tokens(&input_ids, 151670, 151671, 2)?; // OK: 2 embed, 1 rerank
/// ```
pub fn validate_special_tokens(
    input_ids: &[u32],
    embed_token_id: u32,
    rerank_token_id: u32,
    expected_doc_count: usize,
) -> Result<()> {
    let embed_count = input_ids.iter().filter(|&&id| id == embed_token_id).count();

    if embed_count != expected_doc_count {
        return Err(anyhow!(
            "Special token validation failed: Expected {} <|embed_token|> (ID: {}), found {}. \
             This may indicate prompt injection or tokenization error.",
            expected_doc_count,
            embed_token_id,
            embed_count
        ));
    }

    let rerank_count = input_ids.iter().filter(|&&id| id == rerank_token_id).count();

    if rerank_count != 1 {
        return Err(anyhow!(
            "Special token validation failed: Expected exactly 1 <|rerank_token|> (ID: {}), found {}. \
             This may indicate prompt injection or tokenization error.",
            rerank_token_id,
            rerank_count
        ));
    }

    Ok(())
}

#[cfg(test)]
mod validation_tests {
    use super::*;

    #[test]
    fn test_validate_special_tokens_success() {
        let ids = vec![1, 2, 151670, 3, 151670, 4, 151671, 5];
        assert!(validate_special_tokens(&ids, 151670, 151671, 2).is_ok());
    }

    #[test]
    fn test_validate_special_tokens_missing_embed() {
        let ids = vec![1, 2, 151670, 3, 151671, 4]; // embed í† í° 1ê°œë§Œ
        assert!(validate_special_tokens(&ids, 151670, 151671, 2).is_err());
    }

    #[test]
    fn test_validate_special_tokens_extra_rerank() {
        let ids = vec![1, 151670, 2, 151671, 3, 151671, 4]; // rerank í† í° 2ê°œ
        assert!(validate_special_tokens(&ids, 151670, 151671, 1).is_err());
    }

    #[test]
    fn test_validate_special_tokens_no_rerank() {
        let ids = vec![1, 151670, 2, 151670, 3]; // rerank í† í° ì—†ìŒ
        assert!(validate_special_tokens(&ids, 151670, 151671, 2).is_err());
    }
}
```

---

## Milestone 6: ë¼ìš°í„° í†µí•© - ìˆ˜í•™ ìœ í‹¸ë¦¬í‹°

**íŒŒì¼:** `router/src/listwise/math.rs` (ì‹ ê·œ)

```rust
//! Listwise rerankingì„ ìœ„í•œ ë²¡í„° ìˆ˜í•™ ìœ í‹¸ë¦¬í‹°
//!
//! Cosine similarity, ì •ê·œí™”, ê°€ì¤‘ í‰ê· ì„ ìœ„í•œ ìˆœìˆ˜ í•¨ìˆ˜.

use anyhow::{anyhow, Result};

/// ë‘ ë²¡í„° ê°„ cosine similarity ê³„ì‚°
///
/// ê³µì‹: cos(a, b) = (a Â· b) / (||a||_2 * ||b||_2)
///
/// ì£¼ì˜: ì´ í•¨ìˆ˜ëŠ” dot product ê³„ì‚° ì „ì— ë‚´ë¶€ì ìœ¼ë¡œ L2 ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
/// ë°±ì—”ë“œ projector ì¶œë ¥ì€ ì˜ë„ì ìœ¼ë¡œ ì •ê·œí™”ë˜ì§€ ì•ŠìŒ - ì •ê·œí™”ëŠ” ì—¬ê¸°ì„œ ë°œìƒí•©ë‹ˆë‹¤.
/// ì´ê²ƒì€ normalize()ê°€ compute_scores() ë‚´ì—ì„œ í˜¸ì¶œë˜ëŠ” modeling.pyì™€ ì¼ì¹˜í•©ë‹ˆë‹¤.
///
/// # ì¸ì
/// * `a` - ì²« ë²ˆì§¸ ë²¡í„° (ë‚´ë¶€ì ìœ¼ë¡œ ì •ê·œí™”ë¨)
/// * `b` - ë‘ ë²ˆì§¸ ë²¡í„° (ë‚´ë¶€ì ìœ¼ë¡œ ì •ê·œí™”ë¨, `a`ì™€ ê°™ì€ ê¸¸ì´ì—¬ì•¼ í•¨)
///
/// # ë°˜í™˜
/// [-1, 1] ë²”ìœ„ì˜ Cosine similarity
///
/// # ì—ëŸ¬
/// ë²¡í„° ê¸¸ì´ê°€ ë‹¤ë¥´ë©´ ì—ëŸ¬ ë°˜í™˜
pub fn cosine_similarity(a: &[f32], b: &[f32]) -> Result<f32> {
    if a.len() != b.len() {
        return Err(anyhow!(
            "Vector length mismatch: {} vs {}",
            a.len(),
            b.len()
        ));
    }

    if a.is_empty() {
        return Err(anyhow!("Cannot compute cosine of empty vectors"));
    }

    let dot_product: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();

    let norm_a: f32 = a.iter().map(|x| x * x).sum::<f32>().sqrt();
    let norm_b: f32 = b.iter().map(|x| x * x).sum::<f32>().sqrt();

    const EPS: f32 = 1e-8;
    let norm_a = norm_a + EPS;
    let norm_b = norm_b + EPS;

    let similarity = dot_product / (norm_a * norm_b);

    // ìœ íš¨ ë²”ìœ„ë¡œ clamp (ìˆ˜ì¹˜ ì•ˆì •ì„±)
    Ok(similarity.clamp(-1.0, 1.0))
}

/// ë²¡í„°ë¥¼ ì œìë¦¬ì—ì„œ L2 ì •ê·œí™”
///
/// ê³µì‹: x := x / (||x||_2 + eps)
///
/// # ì¸ì
/// * `vec` - ì •ê·œí™”í•  ë²¡í„° (ì œìë¦¬ì—ì„œ ìˆ˜ì •ë¨)
///
/// # ë°˜í™˜
/// ì›ë˜ ë²¡í„°ì˜ L2 norm
pub fn normalize(vec: &mut [f32]) -> f32 {
    const EPS: f32 = 1e-8;

    let norm: f32 = vec.iter().map(|x| x * x).sum::<f32>().sqrt();
    let norm_with_eps = norm + EPS;

    for x in vec.iter_mut() {
        *x /= norm_with_eps;
    }

    norm
}

/// ë²¡í„°ë¥¼ L2 ì •ê·œí™”í•˜ì—¬ ìƒˆ ë²¡í„° ë°˜í™˜
pub fn normalize_new(vec: &[f32]) -> Vec<f32> {
    let mut result = vec.to_vec();
    normalize(&mut result);
    result
}

/// ë²¡í„°ë“¤ì˜ ê°€ì¤‘ í‰ê·  ê³„ì‚°
///
/// ê³µì‹: result = Î£(weight_i * vec_i) / Î£(weight_i)
///
/// # ì¸ì
/// * `vectors` - ë²¡í„°ë“¤ì˜ ìŠ¬ë¼ì´ìŠ¤ (ëª¨ë‘ ê°™ì€ ê¸¸ì´ì—¬ì•¼ í•¨)
/// * `weights` - ê° ë²¡í„°ì˜ ê°€ì¤‘ì¹˜ (ê¸¸ì´ = vectors.len()ì´ì–´ì•¼ í•¨)
///
/// # ë°˜í™˜
/// ê°€ì¤‘ í‰ê·  ë²¡í„°
///
/// # ì—ëŸ¬
/// ë‹¤ìŒ ê²½ìš° ì—ëŸ¬ ë°˜í™˜:
/// - `vectors`ê°€ ë¹„ì–´ìˆìŒ
/// - `weights.len() != vectors.len()`
/// - ë²¡í„°ë“¤ì˜ ê¸¸ì´ê°€ ì¼ê´€ë˜ì§€ ì•ŠìŒ
/// - ê°€ì¤‘ì¹˜ì˜ í•©ì´ ë„ˆë¬´ ì‘ìŒ (< 1e-8)
pub fn weighted_average(vectors: &[Vec<f32>], weights: &[f32]) -> Result<Vec<f32>> {
    if vectors.is_empty() {
        return Err(anyhow!("Cannot compute weighted average of empty vector set"));
    }

    if vectors.len() != weights.len() {
        return Err(anyhow!(
            "Mismatch: {} vectors but {} weights",
            vectors.len(),
            weights.len()
        ));
    }

    let dim = vectors[0].len();
    if dim == 0 {
        return Err(anyhow!("Vectors must have non-zero dimension"));
    }

    // ëª¨ë“  ë²¡í„°ê°€ ê°™ì€ ì°¨ì›ì¸ì§€ í™•ì¸
    for (i, vec) in vectors.iter().enumerate() {
        if vec.len() != dim {
            return Err(anyhow!(
                "Vector {} has length {}, expected {}",
                i,
                vec.len(),
                dim
            ));
        }
    }

    // ê°€ì¤‘ í•© ê³„ì‚°
    let mut result = vec![0.0f32; dim];
    for (vec, &weight) in vectors.iter().zip(weights.iter()) {
        for (r, &v) in result.iter_mut().zip(vec.iter()) {
            *r += weight * v;
        }
    }

    // ê°€ì¤‘ì¹˜ í•©ìœ¼ë¡œ ì •ê·œí™”
    let weight_sum: f32 = weights.iter().sum();
    const EPS: f32 = 1e-8;
    if weight_sum < EPS {
        return Err(anyhow!("Sum of weights too small: {}", weight_sum));
    }

    for r in result.iter_mut() {
        *r /= weight_sum;
    }

    Ok(result)
}

/// ìŠ¤ì¼€ì¼ëœ ë²¡í„° ë”í•˜ê¸°: a := a + scale * b
///
/// # ì¸ì
/// * `a` - ëŒ€ìƒ ë²¡í„° (ì œìë¦¬ì—ì„œ ìˆ˜ì •ë¨)
/// * `b` - ì†ŒìŠ¤ ë²¡í„°
/// * `scale` - ìŠ¤ì¼€ì¼ë§ ì¸ì
///
/// # ì—ëŸ¬
/// ë²¡í„° ê¸¸ì´ê°€ ë‹¤ë¥´ë©´ ì—ëŸ¬ ë°˜í™˜
pub fn add_scaled(a: &mut [f32], b: &[f32], scale: f32) -> Result<()> {
    if a.len() != b.len() {
        return Err(anyhow!(
            "Vector length mismatch: {} vs {}",
            a.len(),
            b.len()
        ));
    }

    for (a_i, &b_i) in a.iter_mut().zip(b.iter()) {
        *a_i += scale * b_i;
    }

    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_cosine_similarity_orthogonal() {
        let a = vec![1.0, 0.0, 0.0];
        let b = vec![0.0, 1.0, 0.0];
        let sim = cosine_similarity(&a, &b).unwrap();
        assert!((sim - 0.0).abs() < 1e-6);
    }

    #[test]
    fn test_cosine_similarity_parallel() {
        let a = vec![1.0, 2.0, 3.0];
        let b = vec![2.0, 4.0, 6.0]; // aì™€ í‰í–‰
        let sim = cosine_similarity(&a, &b).unwrap();
        assert!((sim - 1.0).abs() < 1e-6);
    }

    #[test]
    fn test_cosine_similarity_antiparallel() {
        let a = vec![1.0, 0.0];
        let b = vec![-1.0, 0.0];
        let sim = cosine_similarity(&a, &b).unwrap();
        assert!((sim - (-1.0)).abs() < 1e-6);
    }

    #[test]
    fn test_normalize() {
        let mut vec = vec![3.0, 4.0];
        let norm = normalize(&mut vec);
        assert!((norm - 5.0).abs() < 1e-6);
        assert!((vec[0] - 0.6).abs() < 1e-6);
        assert!((vec[1] - 0.8).abs() < 1e-6);
    }

    #[test]
    fn test_weighted_average() {
        let vectors = vec![
            vec![1.0, 0.0],
            vec![0.0, 1.0],
        ];
        let weights = vec![0.3, 0.7];
        let result = weighted_average(&vectors, &weights).unwrap();
        assert!((result[0] - 0.3).abs() < 1e-6);
        assert!((result[1] - 0.7).abs() < 1e-6);
    }

    #[test]
    fn test_weighted_average_equal_weights() {
        let vectors = vec![
            vec![2.0, 4.0],
            vec![4.0, 6.0],
        ];
        let weights = vec![1.0, 1.0];
        let result = weighted_average(&vectors, &weights).unwrap();
        // í‰ê· : (2+4)/2=3, (4+6)/2=5
        assert!((result[0] - 3.0).abs() < 1e-6);
        assert!((result[1] - 5.0).abs() < 1e-6);
    }

    #[test]
    fn test_add_scaled() {
        let mut a = vec![1.0, 2.0];
        let b = vec![3.0, 4.0];
        add_scaled(&mut a, &b, 0.5).unwrap();
        // a + 0.5*b = [1+1.5, 2+2] = [2.5, 4.0]
        assert!((a[0] - 2.5).abs() < 1e-6);
        assert!((a[1] - 4.0).abs() < 1e-6);
    }

    #[test]
    fn test_cosine_length_mismatch() {
        let a = vec![1.0, 2.0];
        let b = vec![1.0, 2.0, 3.0];
        assert!(cosine_similarity(&a, &b).is_err());
    }

    #[test]
    fn test_weighted_average_length_mismatch() {
        let vectors = vec![vec![1.0, 2.0], vec![3.0, 4.0]];
        let weights = vec![1.0]; // ì˜ëª»ëœ ê¸¸ì´
        assert!(weighted_average(&vectors, &weights).is_err());
    }
}
```

---

## Milestone 7: í ê²©ë¦¬ ë° Prometheus ë©”íŠ¸ë¦­

### 7.1 í ê²©ë¦¬ ì •ì±…

**í˜„ì¬ ë””ìì¸ (V1):**
- Listwise rerankingì€ ë³„ë„ì˜ `BackendCommand::EmbedListwise` variant ì‚¬ìš©
- **ìš”ì²­ ê°„ ë°°ì¹˜ ì—†ìŒ**: ê° ìš”ì²­ì˜ ë¸”ë¡ì€ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ë¨
- **ê³µìœ  ì›Œì»¤ í**: Pairwise ë° listwise ëª…ë ¹ ëª¨ë‘ ê°™ì€ `BackendThread` ì›Œì»¤ë¥¼ í†µê³¼
- ì‹¤í–‰ ìˆœì„œëŠ” ë„ì°© ìˆœì„œì— ë”°ë¼ pairwiseì™€ listwise ìš”ì²­ì´ ì„ì¼ ìˆ˜ ìˆìŒ

**ê·¼ê±°:**
- ë‹¤ë¥¸ ì‚¬ìš©ìì˜ ë¬¸ì„œê°€ ê°™ì€ ì»¨í…ìŠ¤íŠ¸ ì°½ì—ì„œ ìƒí˜¸ì‘ìš©í•˜ëŠ” ê²ƒ ë°©ì§€ (í”„ë¼ì´ë²„ì‹œ/ì •í™•ì„±)
- êµ¬í˜„ ë‹¨ìˆœí™” (ìš”ì²­ ê·¸ë£¹í™” ë¡œì§ ë¶ˆí•„ìš”)
- ì¼ë°˜ì ì¸ ì›Œí¬ë¡œë“œì— ëŒ€í•´ í—ˆìš© ê°€ëŠ¥í•œ ì§€ì—° (ëŒ€ë¶€ë¶„ ìš”ì²­ì´ <125ê°œ ë¬¸ì„œ = 1ë¸”ë¡)

**ë¯¸ë˜ ìµœì í™” (V2):**
Listwise ìš”ì²­ì´ ì§€ë°°ì ì´ê³  pairwise ì§€ì—° ìŠ¤íŒŒì´í¬ë¥¼ ì¼ìœ¼í‚¤ë©´ ê³ ë ¤:
- Listwiseìš© ë³„ë„ ì›Œì»¤ ìŠ¤ë ˆë“œ í’€ (ì‹¤í–‰ ê²©ë¦¬)
- ìš°ì„ ìˆœìœ„ í (pairwiseê°€ ì €ì§€ì—°ì„ ìœ„í•´ ë” ë†’ì€ ìš°ì„ ìˆœìœ„)
- ëª¨ë¸ë‹¹ ì›Œì»¤ í’€ (ë©€í‹° ëª¨ë¸ ì„œë¹™ì„ ìœ„í•´ ì´ë¯¸ ê³„íšë¨)

**ë¬¸ì„œ ì£¼ì˜:**
"Listwise rerankingì— ëŒ€í•´ ìš”ì²­ ê°„ ë°°ì¹˜ëŠ” ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê° ìš”ì²­ì€ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì§€ë§Œ,
pairwiseì™€ listwise ìš”ì²­ì€ ê°™ì€ ë°±ì—”ë“œ ì›Œì»¤ íë¥¼ ê³µìœ í•˜ë©° ì„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

### 7.2 Prometheus ë©”íŠ¸ë¦­ ë“±ë¡

**íŒŒì¼:** `router/src/prometheus.rs`
**ìœ„ì¹˜:** ê¸°ì¡´ `lazy_static!` ë¸”ë¡ì— ì¶”ê°€

âš ï¸ **NIT 5: ë©”íŠ¸ë¦­ ë‹¨ìœ„ ëª…ì‹œì  ë¬¸ì„œí™”**

```rust
use prometheus::{register_histogram, register_int_counter, Histogram, IntCounter};

lazy_static! {
    // ... ê¸°ì¡´ ë©”íŠ¸ë¦­ë“¤ ...

    // Listwise reranker ë©”íŠ¸ë¦­ - ëŒ€ì‹œë³´ë“œ ëª…í™•ì„±ì„ ìœ„í•´ ë‹¨ìœ„ ë¬¸ì„œí™”

    // ë‹¨ìœ„: ë°€ë¦¬ì´ˆ (ms)
    // í† í¬ë‚˜ì´ì œì´ì…˜ë¶€í„° ì ìˆ˜ ê³„ì‚°ê¹Œì§€ ë¸”ë¡ ì²˜ë¦¬ ì§€ì—° ê¸°ë¡
    pub static ref LBNL_MS_PER_GROUP: Histogram = register_histogram!(
        "tei_lbnl_ms_per_group",
        "Latency per listwise block in milliseconds (unit: ms)"
    ).unwrap();

    // ë‹¨ìœ„: í† í° ê°œìˆ˜ (ë¬´ì°¨ì›)
    // í”„ë¡¬í”„íŠ¸ êµ¬ì„± í›„ ì „ì²´ ì‹œí€€ìŠ¤ ê¸¸ì´ ê¸°ë¡
    pub static ref LBNL_SEQ_TOKENS: Histogram = register_histogram!(
        "tei_lbnl_seq_tokens",
        "Total tokens in listwise block sequence (unit: tokens)"
    ).unwrap();

    // ë‹¨ìœ„: ë¬¸ì„œ ê°œìˆ˜ (ë¬´ì°¨ì›)
    // ê° ë¸”ë¡ì—ì„œ ì²˜ë¦¬ëœ ë¬¸ì„œ ê°œìˆ˜ ê¸°ë¡ (ìµœëŒ€: 125)
    pub static ref LBNL_GROUP_SIZE: Histogram = register_histogram!(
        "tei_lbnl_group_size",
        "Number of documents in listwise block (unit: count, max: 125)"
    ).unwrap();

    // ë‹¨ìœ„: ê°œìˆ˜ (ì¹´ìš´í„° ì¦ê°€)
    // ë¸”ë¡ ì²˜ë¦¬ê°€ íƒ€ì„ì•„ì›ƒ ì„ê³„ê°’ì„ ì´ˆê³¼í•  ë•Œë§ˆë‹¤ ì¦ê°€
    pub static ref LBNL_BLOCK_TIMEOUT_TOTAL: IntCounter = register_int_counter!(
        "tei_lbnl_block_timeout_total",
        "Total number of listwise block processing timeouts (unit: count)"
    ).unwrap();
}
```

> **ì¤‘ìš”:** TEIëŠ” `metrics::` crateê°€ ì•„ë‹Œ Prometheus `lazy_static!` ë ˆì§€ìŠ¤íŠ¸ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
> ëª¨ë“  í•¸ë“¤ëŸ¬ ì½”ë“œëŠ” ì´ëŸ¬í•œ static refë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤ (ì˜ˆ: `LBNL_MS_PER_GROUP.observe(...)`)

> **NIT 5 - ë©”íŠ¸ë¦­ ë‹¨ìœ„ ìš”ì•½:**
> - `tei_lbnl_ms_per_group`: **ë°€ë¦¬ì´ˆ** (ì§€ì—°)
> - `tei_lbnl_seq_tokens`: **í† í°** (ì‹œí€€ìŠ¤ ê¸¸ì´)
> - `tei_lbnl_group_size`: **ê°œìˆ˜** (ë¸”ë¡ë‹¹ ë¬¸ì„œ, ìµœëŒ€ 125)
> - `tei_lbnl_block_timeout_total`: **ê°œìˆ˜** (íƒ€ì„ì•„ì›ƒ ì´ë²¤íŠ¸)
>
> ì´ëŸ¬í•œ ë‹¨ìœ„ëŠ” Prometheus ëŒ€ì‹œë³´ë“œ ë° ì•Œë¦¼ ê·œì¹™ì— ì¤‘ìš”í•©ë‹ˆë‹¤.

---

## Milestone 8: ë¼ìš°í„° í•¸ë“¤ëŸ¬ êµ¬í˜„

### 8.1 Listwise Rerank í•¸ë“¤ëŸ¬

**íŒŒì¼:** `router/src/http/server.rs`
**ìœ„ì¹˜:** ìƒˆ í•¸ë“¤ëŸ¬ í•¨ìˆ˜ ì¶”ê°€

ì´ êµ¬í˜„ì€ ì™„ì „í•œ listwise reranking íŒŒì´í”„ë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤. ì½”ë“œê°€ ë§¤ìš° ê¸¸ë¯€ë¡œ ì£¼ìš” ì„¹ì…˜ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì„¤ëª…í•©ë‹ˆë‹¤:

```rust
use axum::{extract::State, http::{HeaderMap, StatusCode}, Json};
use crate::http::types::ErrorResponse;
use std::time::Instant;
use crate::listwise::math::{cosine_similarity, normalize, weighted_average};
use text_embeddings_core::tokenization::{encode_listwise, truncate_texts, validate_special_tokens};
use text_embeddings_core::prompt::build_jina_v3_prompt;

/// Listwise rerankingì„ ìœ„í•œ HTTP í•¸ë“¤ëŸ¬
///
/// ì™„ì „í•œ listwise reranking íŒŒì´í”„ë¼ì¸ êµ¬í˜„:
/// 1. ì…ë ¥ ê²€ì¦ ë° í˜ì´ë¡œë“œ ì œí•œ í™•ì¸
/// 2. í…ìŠ¤íŠ¸ë¥¼ í† í° ì œí•œìœ¼ë¡œ ì ˆë‹¨
/// 3. í† í° ì˜ˆì‚°ì„ ê³ ë ¤í•˜ì—¬ ë¸”ë¡ êµ¬ì„±
/// 4. ê° ë¸”ë¡ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
/// 5. ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì¿¼ë¦¬ ì„ë² ë”© ì—…ë°ì´íŠ¸
/// 6. ìˆœìœ„ê°€ ë§¤ê²¨ì§„ ê²°ê³¼ ë°˜í™˜
pub async fn rerank_listwise(
    State(state): State<AppState>,
    Json(req): Json<RerankRequest>,
) -> Result<(HeaderMap, Json<RerankResponse>), (StatusCode, Json<ErrorResponse>)> {
    let start = Instant::now();

    // ìš”ì²­ ê²€ì¦
    if req.texts.is_empty() {
        return Err((
            StatusCode::BAD_REQUEST,
            Json(ErrorResponse { error: "texts array cannot be empty".to_string(), error_type: "invalid_input".into() })
        ));
    }

    let config = &state.listwise_config;
    if req.texts.len() > config.max_documents_per_request {
        return Err((
            StatusCode::BAD_REQUEST,
            Json(ErrorResponse { error: format!(
                "Too many documents: {} (max: {})",
                req.texts.len(),
                config.max_documents_per_request
            ), error_type: "invalid_input".into() })
        ));
    }

    for (i, doc) in req.texts.iter().enumerate() {
        if doc.len() > config.max_document_length_bytes {
            return Err((
                StatusCode::BAD_REQUEST,
                Json(ErrorResponse { error: format!(
                    "Document {} exceeds maximum length: {} > {}",
                    i,
                    doc.len(),
                    config.max_document_length_bytes
                ), error_type: "invalid_input".into() })
            ));
        }
    }

    // í† í¬ë‚˜ì´ì € ë° íŠ¹ìˆ˜ í† í° ID ê°€ì ¸ì˜¤ê¸°
    let tokenizer = state.infer.tokenizer();
    let embed_token_id = tokenizer
        .token_to_id("<|embed_token|>")
        .ok_or((StatusCode::INTERNAL_SERVER_ERROR, Json(ErrorResponse { error: "Missing embed_token".to_string(), error_type: "tokenizer".into() })))?;
    let rerank_token_id = tokenizer
        .token_to_id("<|rerank_token|>")
        .ok_or((StatusCode::INTERNAL_SERVER_ERROR, Json(ErrorResponse { error: "Missing rerank_token".to_string(), error_type: "tokenizer".into() })))?;

    // 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì ˆë‹¨
    let (query_truncated, docs_truncated, doc_lengths, query_length) = truncate_texts(
        tokenizer,
        &req.query,
        &req.texts,
        512,  // max_query_length
        2048, // max_doc_length
    )
    .map_err(|e| (StatusCode::INTERNAL_SERVER_ERROR, Json(ErrorResponse { error: e.to_string(), error_type: "tokenizer".into() })))?;

    // 2ë‹¨ê³„: ì‚¬ì „ ê³„ì‚°ëœ í† í° ê¸¸ì´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¸”ë¡ êµ¬ì„± (ì¬ì¸ì½”ë”© ì—†ìŒ)
    let max_length = tokenizer
        .get_truncation()
        .map(|t| t.max_length)
        .unwrap_or(state.info.max_input_length);
    let mut capacity = max_length.saturating_sub(2 * query_length);
    let mut all_doc_embeddings = Vec::with_capacity(docs_truncated.len());
    let mut all_doc_indices    = Vec::with_capacity(docs_truncated.len());
    let mut all_query_embeddings = Vec::new();
    let mut all_block_weights = Vec::new();

    let mut current_block_docs = Vec::new();
    let mut current_block_indices = Vec::new();

    // ìˆœì„œ ì ìš© (input|random)
    let mut order: Vec<usize> = (0..docs_truncated.len()).collect();
    if matches!(config.ordering, RerankOrdering::Random) {
        use rand::{seq::SliceRandom, SeedableRng};
        let mut rng = config
            .random_seed
            .map(rand::rngs::StdRng::seed_from_u64)
            .unwrap_or_else(rand::rngs::StdRng::from_entropy);
        order.shuffle(&mut rng);
        tracing::warn!(
            seed = ?config.random_seed,
            "Using random ordering; results are non-deterministic without a seed."
        );
    }

    // ì¤‘ìš”: ì ˆë‹¨ ë‹¨ê³„ì—ì„œ ì‚¬ì „ ê³„ì‚°ëœ doc_lengths ì‚¬ìš©
    // ì¬ì¸ì½”ë”© ì˜¤ë²„í—¤ë“œë¥¼ í”¼í•˜ê³  ì¼ê´€ëœ ì²­í‚¹ ë¡œì§ ë³´ì¥
    for idx in order {
        let doc = &docs_truncated[idx];
        let doc_token_len = doc_lengths[idx];  // ì ˆë‹¨ëœ í† í° ê¸¸ì´ ì‚¬ìš©
        current_block_docs.push(doc.as_str());
        current_block_indices.push(idx);
        capacity = capacity.saturating_sub(doc_token_len);

        // ë¸”ë¡ì´ ê°€ë“ ì°¨ë©´ í”ŒëŸ¬ì‹œ
        if current_block_docs.len() >= config.max_docs_per_pass || capacity <= 2048 {
            // ì¤‘ìš”: í”„ë¡¬í”„íŠ¸ ì˜¤ë²„í”Œë¡œìš°ë¥¼ ìœ„í•œ shrink-to-fit ì¬ì‹œë„
            // í¬ê·€ ì—£ì§€ ì¼€ì´ìŠ¤: í…œí”Œë¦¿ ì˜¤ë²„í—¤ë“œë¡œ ë¸”ë¡ì´ max_length ì´ˆê³¼
            // í•´ê²°ì±…: ë§ˆì§€ë§‰ ë¬¸ì„œ ì œê±°í•˜ê³  ì¬ì‹œë„, ë‹¤ìŒ ë¸”ë¡ìœ¼ë¡œ ìŠ¤í•„
            let mut retry_docs = current_block_docs.clone();
            let mut retry_indices = current_block_indices.clone();
            let mut spilled_docs = Vec::new();
            let mut spilled_indices = Vec::new();

            let (block_embeds, block_query_emb, block_weight) = loop {
                match process_block(
                    &state,
                    &query_truncated,
                    &retry_docs,
                    config.instruction.as_deref(),
                    embed_token_id,
                    rerank_token_id,
                    config.block_timeout_ms,
                )
                .await
                {
                    Ok(result) => break result,

                    // âš ï¸ ê°•ë ¥ ê¶Œì¥ ìˆ˜ì •: ë‹¨ì¼ ë¬¸ì„œ ì˜¤ë²„í”Œë¡œìš° ëª…ì‹œì  ì²˜ë¦¬
                    Err(ProcessBlockError::Tokenization(msg))
                        if msg.contains("Prompt exceeds max length") && retry_docs.len() == 1 =>
                    {
                        // ë‹¨ì¼ ë¬¸ì„œì¡°ì°¨ ì»¨í…ìŠ¤íŠ¸ ì´ˆê³¼ - ë” ì´ìƒ ì¤„ì¼ ìˆ˜ ì—†ìŒ
                        return Err((
                            StatusCode::UNPROCESSABLE_ENTITY,
                            Json(ErrorResponse {
                                error: format!(
                                    "Single document block still exceeds model context limit. \
                                     Document may be too long even after truncation ({}). \
                                     Try reducing document length or using a model with larger context.",
                                    state.info.max_input_length
                                ),
                                error_type: "token_limit_exceeded".into(),
                            }),
                        ));
                    }

                    Err(ProcessBlockError::Tokenization(msg))
                        if msg.contains("Prompt exceeds max length") && retry_docs.len() > 1 =>
                    {
                        // ë¸”ë¡ ì¶•ì†Œ: ë§ˆì§€ë§‰ ë¬¸ì„œë¥¼ ìŠ¤í•„ ë²„í¼ë¡œ ì´ë™
                        let spill_doc = retry_docs.pop().unwrap();
                        let spill_idx = retry_indices.pop().unwrap();
                        spilled_docs.insert(0, spill_doc);
                        spilled_indices.insert(0, spill_idx);
                        tracing::warn!(
                            "Block overflow: shrinking from {} to {} docs, spilling 1 to next block",
                            retry_docs.len() + 1,
                            retry_docs.len()
                        );
                        continue; // ë” ì‘ì€ ë¸”ë¡ìœ¼ë¡œ ì¬ì‹œë„
                    }
                    Err(e) => return Err(map_process_error(e)),
                }
            };

            all_doc_embeddings.extend(block_embeds);
            all_doc_indices.extend(retry_indices.iter().copied());
            all_query_embeddings.push(block_query_emb);
            all_block_weights.push(block_weight);

            current_block_docs.clear();
            current_block_indices.clear();

            // ì¤‘ìš”: ìŠ¤í•„ëœ ë¬¸ì„œë¥¼ ë‹¤ìŒ ë¸”ë¡ ì•ì— ì¶”ê°€í•˜ê³  capacity ì¬ê³„ì‚°
            current_block_docs.extend(spilled_docs);
            current_block_indices.extend(spilled_indices.iter().copied());

            // ìŠ¤í•„ëœ ë¬¸ì„œë¥¼ ê³ ë ¤í•˜ì—¬ capacity ì¬ê³„ì‚°
            capacity = max_length.saturating_sub(2 * query_length);
            for &idx in &current_block_indices {
                capacity = capacity.saturating_sub(doc_lengths[idx]);
            }
        }
    }

    // ë‚¨ì€ ë¬¸ì„œ ì²˜ë¦¬ (ê°™ì€ shrink-to-fit ì¬ì‹œë„ ì‚¬ìš©)
    if !current_block_docs.is_empty() {
        let mut retry_docs = current_block_docs.clone();
        let mut retry_indices = current_block_indices.clone();

        let (block_embeds, block_query_emb, block_weight) = loop {
            match process_block(
                &state,
                &query_truncated,
                &retry_docs,
                config.instruction.as_deref(),
                embed_token_id,
                rerank_token_id,
                config.block_timeout_ms,
            )
            .await
            {
                Ok(result) => break result,
                Err(ProcessBlockError::Tokenization(msg))
                    if msg.contains("Prompt exceeds max length") && retry_docs.len() > 1 =>
                {
                    retry_docs.pop();
                    retry_indices.pop();
                    tracing::warn!("Final block overflow: shrinking to {} docs", retry_docs.len());
                    continue;
                }
                Err(e) => return Err(map_process_error(e)),
            }
        };

        all_doc_embeddings.extend(block_embeds);
        all_doc_indices.extend(retry_indices.iter().copied());
        all_query_embeddings.push(block_query_emb);
        all_block_weights.push(block_weight);
    }

    // 3ë‹¨ê³„: ë¸”ë¡ë³„ ì¿¼ë¦¬ ì„ë² ë”© ì§‘ê³„ ë° ë¬¸ì„œ ì ìˆ˜ ë§¤ê¸°ê¸°
    let final_query_embedding = if all_query_embeddings.len() > 1 {
        weighted_average(&all_query_embeddings, &all_block_weights)
            .map_err(|e| (StatusCode::INTERNAL_SERVER_ERROR, Json(ErrorResponse { error: e.to_string(), error_type: "backend".into() })))?
    } else if !all_query_embeddings.is_empty() {
        all_query_embeddings[0].clone()
    } else {
        return Err((StatusCode::BAD_REQUEST, Json(ErrorResponse { error: "No blocks processed".to_string(), error_type: "invalid_input".into() })));
    };

    debug_assert_eq!(all_doc_embeddings.len(), all_doc_indices.len());

    // ëª¨ë“  ë¬¸ì„œ ì„ë² ë”©ì— ëŒ€í•´ cosine similarity ê³„ì‚°
    let mut scores = Vec::with_capacity(all_doc_embeddings.len());
    for emb in &all_doc_embeddings {
        let score = cosine_similarity(&final_query_embedding, emb).map_err(|e| {
            (
                StatusCode::INTERNAL_SERVER_ERROR,
                Json(ErrorResponse { error: e.to_string(), error_type: "backend".into() }),
            )
        })?;
        scores.push(score);
    }

    let mut pairs: Vec<(usize, f32)> = all_doc_indices
        .iter()
        .copied()
        .zip(scores.into_iter())
        .collect();

    // ì¤‘ìš”: ì¸ë±ìŠ¤ë¡œ íƒ€ì´ ë¸Œë ˆì´í‚¹ + NaN ì²˜ë¦¬ë¡œ ì•ˆì • ì •ë ¬
    // 1. NaN ì ìˆ˜ëŠ” ìµœí•˜ìœ„ë¡œ ì²˜ë¦¬ (ì–´ë–¤ ìœ í•œ ì ìˆ˜ë³´ë‹¤ ë‚˜ì¨)
    // 2. ì ìˆ˜ê°€ ê°™ìœ¼ë©´ ì…ë ¥ ìˆœì„œ ìœ ì§€ (ë‚®ì€ ì¸ë±ìŠ¤ ë¨¼ì €)
    // 3. ì—£ì§€ ì¼€ì´ìŠ¤ê°€ ìˆì–´ë„ ì¬í˜„ ê°€ëŠ¥í•œ ìˆœìœ„ ë³´ì¥
    use std::cmp::Ordering;
    pairs.sort_by(|a, b| {
        match (a.1.is_nan(), b.1.is_nan()) {
            (true, true) => a.0.cmp(&b.0),           // ë‘˜ ë‹¤ NaN: ì¸ë±ìŠ¤ë¡œ íƒ€ì´ ë¸Œë ˆì´í¬
            (true, false) => Ordering::Greater,       // aê°€ NaN: a < b (NaNì´ ìµœì•…)
            (false, true) => Ordering::Less,          // bê°€ NaN: a > b
            (false, false) => {
                // ë‘˜ ë‹¤ NaN ì•„ë‹˜: íƒ€ì´ ë¸Œë ˆì´í¬ë¡œ ì •ìƒ ë¹„êµ
                b.1.partial_cmp(&```rust
                    .unwrap_or(Ordering::Equal)       // ë°œìƒí•˜ì§€ ì•Šì•„ì•¼ í•¨ (ë‘˜ ë‹¤ ìœ í•œ)
                    .then_with(|| a.0.cmp(&b.0))      // íƒ€ì´ ë¸Œë ˆì´í¬: ë‚®ì€ ì¸ë±ìŠ¤ ìŠ¹ë¦¬
            }
        }
    });

    let results = pairs.into_iter().map(|(index, score)| RankResult { index, score }).collect();

    let duration = start.elapsed();
    tracing::info!(
        "Listwise rerank completed: {} docs in {:.2}ms",
        req.texts.len(),
        duration.as_secs_f64() * 1000.0
    );

    // ë””ë²„ê·¸ ì •ë³´ê°€ í¬í•¨ëœ ì‘ë‹µ í—¤ë” êµ¬ì„±
    let mut headers = HeaderMap::new();
    let total_time_ms = start.elapsed().as_millis();
    headers.insert("x-total-time", total_time_ms.to_string().parse().unwrap());

    // ê¶Œì¥: ë””ë²„ê¹…/ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ìš´ì˜ ê°€ì‹œì„± í—¤ë” ì¶”ê°€
    headers.insert("x-tei-rerank-strategy", "listwise".parse().unwrap());
    headers.insert("x-tei-lbnl-blocks", all_query_embeddings.len().to_string().parse().unwrap());
    headers.insert("x-tei-lbnl-docs", req.texts.len().to_string().parse().unwrap());
    headers.insert("x-tei-lbnl-ordering", format!("{:?}", config.ordering).parse().unwrap());
    if let Some(seed) = config.random_seed {
        headers.insert("x-tei-lbnl-seed", seed.to_string().parse().unwrap());
    }

    Ok((headers, Json(RerankResponse { results })))
}

/// ë‹¨ì¼ ë¬¸ì„œ ë¸”ë¡ ì²˜ë¦¬
#[derive(Debug)]
enum ProcessBlockError {
    Tokenization(String),
    Validation(String),
    Timeout,
    Backend(String),
}

fn map_process_error(err: ProcessBlockError) -> (StatusCode, Json<ErrorResponse>) {
    match err {
        ProcessBlockError::Tokenization(msg) => (
            StatusCode::BAD_REQUEST,
            Json(ErrorResponse { error: msg, error_type: "tokenizer".into() }),
        ),
        ProcessBlockError::Validation(msg) => (
            StatusCode::UNPROCESSABLE_ENTITY,
            Json(ErrorResponse { error: msg, error_type: "invalid_input".into() }),
        ),
        ProcessBlockError::Timeout => (
            StatusCode::GATEWAY_TIMEOUT,
            Json(ErrorResponse {
                error: "Block processing timeout".to_string(),
                error_type: "backend".into(),
            }),
        ),
        ProcessBlockError::Backend(msg) => (
            StatusCode::INTERNAL_SERVER_ERROR,
            Json(ErrorResponse { error: msg, error_type: "backend".into() }),
        ),
    }
}

use std::time::Instant;

async fn process_block(
    state: &AppState,
    query: &str,
    docs: &[&str],
    instruction: Option<&str>,
    embed_token_id: u32,
    rerank_token_id: u32,
    timeout_ms: u64,
) -> Result<(Vec<Vec<f32>>, Vec<f32>, f32), ProcessBlockError> {
    let block_start = Instant::now();
    // í”„ë¡¬í”„íŠ¸ ë¹Œë“œ
    let prompt = build_jina_v3_prompt(query, docs, instruction);

    // í† í¬ë‚˜ì´ì¦ˆ
    // ì¤‘ìš”: max_lengthë¥¼ ìœ„í•œ í´ë°± ì²´ì¸ (truncation â†’ ëª¨ë¸ ì„¤ì • â†’ ì—ëŸ¬)
    let max_len = state
        .infer
        .tokenizer()
        .get_truncation()
        .map(|t| t.max_length)
        .or(Some(state.info.max_input_length))
        .filter(|&len| len > 0)  // ìœ íš¨í•œ ê¸¸ì´ ë³´ì¥
        .ok_or_else(|| ProcessBlockError::Tokenization(
            "max input length unavailable from tokenizer or model config".into()
        ))?;

    let encoding = encode_listwise(state.infer.tokenizer(), &prompt, Some(max_len))
        .map_err(|e| ProcessBlockError::Tokenization(e.to_string()))?;
    let total_tokens = encoding.len();

    // ì¤‘ìš”: ë°±ì—”ë“œ ì²˜ë¦¬ ì „ íŠ¹ìˆ˜ í† í° ê°œìˆ˜ ê²€ì¦
    // Hidden statesì—ì„œ ì„ë² ë”© ì¶”ì¶œì‹œ ë²”ìœ„ ë°– ì ‘ê·¼ ë°©ì§€
    validate_special_tokens(
        encoding.get_ids(),
        embed_token_id,
        rerank_token_id,
        docs.len(),
    )
    .map_err(|e| ProcessBlockError::Validation(e.to_string()))?;

    // ì¸ì½”ë”©ì—ì„œ ListwiseBlockInput êµ¬ì„±
    let input_ids: Vec<u32> = encoding.get_ids().to_vec();
    let attention_mask_raw = encoding.get_attention_mask();
    let attention_mask: Vec<u32> = attention_mask_raw.iter().map(|&m| if m > 0 { 1u32 } else { 0u32 }).collect();
    let block_input = ListwiseBlockInput {
        input_ids,
        attention_mask,
        embed_token_id,
        rerank_token_id,
        doc_count: docs.len(),
    };

    // íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ë°±ì—”ë“œ í˜¸ì¶œ: ë°±ì—”ë“œëŠ” ì¿¼ë¦¬ + ë¬¸ì„œ ì„ë² ë”© ëª¨ë‘ ë°˜í™˜
    //
    // âš ï¸ **SHOULD-FIX S4: íƒ€ì„ì•„ì›ƒ ë¹„ì·¨ì†Œ ë¬¸ì„œí™”**
    // ì¤‘ìš”: tokio::time::timeoutì€ ëŒ€ê¸° ì¤‘ì¸ Futureë§Œ ì·¨ì†Œí•˜ë©°, ë°±ì—”ë“œ ê³„ì‚°ì€ ì·¨ì†Œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!
    // ë°±ì—”ë“œ ì›Œì»¤ ìŠ¤ë ˆë“œëŠ” íƒ€ì„ì•„ì›ƒ í›„ì—ë„ ê³„ì† ì²˜ë¦¬í•©ë‹ˆë‹¤. ì´ê²ƒì€ í—ˆìš© ê°€ëŠ¥í•©ë‹ˆë‹¤:
    // 1. ë°±ì—”ë“œ ì‘ì—…ì€ ê²©ë¦¬ë¨ (ê³µìœ  ê°€ë³€ ìƒíƒœ ì—†ìŒ)
    // 2. ë‚­ë¹„ëœ ê³„ì‚°ì€ ë‹¨ì¼ ë¸”ë¡ í¬ê¸°ë¡œ ì œí•œë¨
    // 3. ë©”íŠ¸ë¦­ì´ ìš©ëŸ‰ ê³„íšì„ ìœ„í•œ íƒ€ì„ì•„ì›ƒ ë¹ˆë„ ì¶”ì 
    //
    // ë¯¸ë˜ ê°œì„ : ì·¨ì†Œê°€ í•„ìš”í•˜ë©´ ë‹¤ìŒì„ ì‚¬ìš©í•˜ì—¬ í‚¬ ìŠ¤ìœ„ì¹˜ êµ¬í˜„:
    // - ì·¨ì†Œ ì‹ í˜¸ë¥¼ ìœ„í•œ oneshot ì±„ë„
    // - ë°±ì—”ë“œê°€ ë¹„ìš©ì´ í° ì‘ì—… ì „ì— ì·¨ì†Œ í† í° í™•ì¸
    // - í˜„ì¬ ë””ìì¸ì€ ì·¨ì†Œ ë³µì¡ì„±ë³´ë‹¤ ë‹¨ìˆœì„± ìš°ì„ 
    let output = tokio::time::timeout(
        std::time::Duration::from_millis(timeout_ms),
        state.infer.embed_listwise_block(block_input),
    )
    .await
    .map_err(|_| {
        // ëª¨ë‹ˆí„°ë§ì„ ìœ„í•´ íƒ€ì„ì•„ì›ƒ ë°œìƒ ì¶”ì  (Prometheus ë ˆì§€ìŠ¤íŠ¸ë¦¬)
        use crate::prometheus::LBNL_BLOCK_TIMEOUT_TOTAL;
        LBNL_BLOCK_TIMEOUT_TOTAL.inc();
        ProcessBlockError::Timeout
    })?
    .map_err(|e| ProcessBlockError::Backend(e.to_string()))?;

    let query_emb = output.query_embedding;
    let doc_embeds = output.doc_embeddings;

    // ì¤‘ìš”: TEIì˜ ê¸°ì¡´ Prometheus ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì‚¬ìš© (router/src/prometheus.rsì— ì •ì˜)
    // metrics:: crate ì•„ë‹˜ - prometheus.rsì˜ LBNL_* ë©”íŠ¸ë¦­ ì •ì˜ ì°¸ì¡°
    use crate::prometheus::{LBNL_MS_PER_GROUP, LBNL_SEQ_TOKENS, LBNL_GROUP_SIZE};

    LBNL_MS_PER_GROUP.observe(block_start.elapsed().as_secs_f64() * 1000.0);
    LBNL_SEQ_TOKENS.observe(total_tokens as f64);
    LBNL_GROUP_SIZE.observe(docs.len() as f64);

    // ì´ ë¸”ë¡ì˜ ì¿¼ë¦¬ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ìœ„í•œ ë¸”ë¡ ì ìˆ˜ ê³„ì‚°
    let mut block_scores = Vec::with_capacity(doc_embeds.len());
    for emb in &doc_embeds {
        let score = cosine_similarity(&query_emb, emb)
            .map_err(|e| ProcessBlockError::Backend(e.to_string()))?;
        block_scores.push(score);
    }

    // ê°€ì¤‘ì¹˜ëŠ” ìµœëŒ€ ì •ê·œí™” ì ìˆ˜: (1 + max_score) / 2
    // ì¤‘ìš”: ìˆ˜ì¹˜ ë¶ˆì•ˆì •ì„±ìœ¼ë¡œ ì¸í•œ NaN/Inf ë°©ì§€
    let max_score = block_scores
        .iter()
        .copied()
        .filter(|s| s.is_finite())  // NaN ë° Â±Inf í•„í„°ë§
        .fold(f32::NEG_INFINITY, f32::max);

    if !max_score.is_finite() {
        return Err(ProcessBlockError::Backend(
            "All block scores are invalid (NaN or Inf). Check input data.".into()
        ));
    }

    // ê°€ì¤‘ì¹˜ë¥¼ ìœ íš¨ ë²”ìœ„ [0, 1]ë¡œ clampí•˜ê³  ì œë¡œ ê°€ì¤‘ì¹˜ ë¸”ë¡ ë°©ì§€ë¥¼ ìœ„í•´ floor ì ìš©
    let mut weight = ((1.0 + max_score).clamp(-1.0, 1.0)) / 2.0;
    if weight <= 1e-8 {
        weight = 1e-6;  // FloorëŠ” weighted_averageì—ì„œ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
    }

    Ok((doc_embeds, query_emb, weight))
}

/// ìš”ì²­/ì‘ë‹µ íƒ€ì…
#[derive(Debug, serde::Deserialize)]
pub struct RerankRequest {
    pub query: String,
    pub texts: Vec<String>,
}

#[derive(Debug, serde::Serialize)]
pub struct RerankResponse {
    pub results: Vec<RankResult>,
}

#[derive(Debug, serde::Serialize)]
pub struct RankResult {
    pub index: usize,
    pub score: f32,
}
```

### 8.2 `/rerank` ë¼ìš°íŠ¸ ì—°ê²°

**íŒŒì¼:** `router/src/http/server.rs`
**ìœ„ì¹˜:** ê¸°ì¡´ `/rerank` í•¸ë“¤ëŸ¬ ë‚´ë¶€, ì‘ë‹µ ë°˜í™˜ ì§ì „

```rust
let strategy = state.determine_strategy().map_err(|e| {
    (StatusCode::BAD_REQUEST, Json(ErrorResponse { error: e.to_string(), error_type: "invalid_input".into() }))
})?;

match strategy {
    RerankStrategy::Listwise => rerank_listwise(State(state.clone()), Json(req)).await,
    RerankStrategy::Pairwise => {
        let pairwise = rerank_pairwise(State(state), Json(req)).await?;
        Ok((HeaderMap::new(), pairwise))
    }
}
```

> Pairwise ê²½ë¡œëŠ” ê¸°ì¡´ TEI êµ¬í˜„ì…ë‹ˆë‹¤. ìƒˆë¡œìš´ listwise ë¸Œëœì¹˜ëŠ” ìœ„ì— ì •ì˜ëœ í•¸ë“¤ëŸ¬ë¥¼ ì¬ì‚¬ìš©í•˜ë©° ë‹¤ë¥¸ ëª¨ë“  ì½”ë“œëŠ” ë³€ê²½ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

---

## Milestone 9: End-to-End í†µí•©

### 9.1 ì™„ì „í•œ í†µí•© íë¦„

```rust
// íŒŒì¼: integration_tests/tests/listwise_rerank.rs

use text_embeddings_inference::*;

#[tokio::test]
async fn test_listwise_rerank_end_to_end() {
    // 1. ëª¨ë¸ ì´ˆê¸°í™”
    let model_path = Path::new("jinaai/jina-reranker-v3");
    let tokenizer = Tokenizer::from_file(model_path.join("tokenizer.json"))
        .expect("Failed to load tokenizer.json");

    // 2. ëª¨ë¸ ì¢…ë¥˜ ê°ì§€
    let model_kind = detect_model_kind(Path::new(model_path), &tokenizer).unwrap();
    assert_eq!(model_kind, ModelKind::ListwiseReranker);

    // 3. Listwise ì„¤ì • êµ¬ì„±
    let config = ListwiseConfig {
        max_docs_per_pass: 125,
        ordering: RerankOrdering::Input,
        instruction: None,
        payload_limit_bytes: 2_000_000,
        block_timeout_ms: 30_000,
        random_seed: Some(42),
        max_documents_per_request: 1_000,
        max_document_length_bytes: 102_400,
    };

    // 4. App state ìƒì„±
    let infer = Infer::new(/* backend */);
    let info = Info::new(/* metadata */);
    let state = AppState::new(infer, info, model_kind, RerankMode::Auto, config);

    // 5. Rerank ìš”ì²­ ì „ì†¡
    let request = RerankRequest {
        query: "What is machine learning?".to_string(),
        texts: vec![
            "Machine learning is a subset of AI.".to_string(),
            "Python is a programming language.".to_string(),
            "Deep learning uses neural networks.".to_string(),
        ],
    };

    // âš ï¸ ì¤‘ìš” ìˆ˜ì •: ì˜¬ë°”ë¥¸ ì‘ë‹µ ì–¸íŒ¨í‚¹
    // rerank_listwiseëŠ” (HeaderMap, Json<RerankResponse>) ë°˜í™˜, Json ì§ì ‘ ì•„ë‹˜
    let (headers, Json(body)) = rerank_listwise(State(state), Json(request)).await.unwrap();

    // 6. ê²°ê³¼ ê²€ì¦
    assert_eq!(body.results.len(), 3);
    assert!(body.results[0].score > body.results[1].score);

    // ML ê´€ë ¨ ë¬¸ì„œê°€ ë” ë†’ê²Œ ìˆœìœ„ ë§¤ê²¨ì§ˆ ê²ƒìœ¼ë¡œ ì˜ˆìƒ
    assert!(body.results.iter().any(|r| r.index == 0)); // ML ë¬¸ì„œ
    assert!(body.results.iter().any(|r| r.index == 2)); // DL ë¬¸ì„œ

    // í—¤ë” ì¡´ì¬ í™•ì¸
    assert!(headers.contains_key("x-total-time"));
}
```

### 9.2 ë¼ìš°í„° Listwise ëª¨ë“ˆ êµ¬ì„±

**íŒŒì¼:** `router/src/listwise/mod.rs` (ì‹ ê·œ)

```rust
pub mod math;
pub use math::*;
```

### 9.3 Infer í†µí•© (ê¸€ë£¨)

**íŒŒì¼:** `core/src/infer.rs`

```rust
use text_embeddings_backend_core::{ListwiseBlockInput, ListwiseBlockOutput};
use tokio::sync::oneshot;
use tracing::{instrument, Span};

impl Infer {
    /// ë°°ì¹˜ íë¥¼ ê±°ì¹˜ì§€ ì•Šê³  ë°±ì—”ë“œì— listwise ë¸”ë¡ ë””ìŠ¤íŒ¨ì¹˜.
    ///
    /// ì¤‘ìš”: ì´ ë©”ì†Œë“œëŠ” ì´ì „ì— `Backend::embed_listwise_block()`ì— ìˆë˜
    /// ì±„ë„ ë””ìŠ¤íŒ¨ì¹˜ ë¡œì§(oneshot sender/receiver)ì„ í¬í•¨í•©ë‹ˆë‹¤. ì—¬ê¸°ì— ì¤‘ì•™í™”í•˜ë©´
    /// ì¤‘ë³µì„ í”¼í•˜ê³  í•œ ê³³ì— ë¹„ë™ê¸° ê²½ê³„ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
    ///
    /// âš ï¸ **ë¸”ë¡œì»¤ B2 ìˆ˜ì • ì ìš©:** ì±„ë„ì´ ê°€ë“ ì°° ë•Œ íŒ¨ë‹‰ì„ ë°©ì§€í•˜ê¸° ìœ„í•´
    /// `try_send()` ëŒ€ì‹  `send().await`ë¥¼ ì‚¬ìš©í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ë°±í”„ë ˆì…” ì ìš©.
    /// íŠ¸ë˜í”½ ê¸‰ì¦ ì¤‘ íŒ¨ë‹‰ì„ ë°©ì§€í•˜ê³  ì‹œìŠ¤í…œì´ ìì²´ ì¡°ì ˆí•˜ë„ë¡ í—ˆìš©.
    #[instrument(skip_all)]
    pub async fn embed_listwise_block(
        &self,
        input: ListwiseBlockInput,
    ) -> Result<ListwiseBlockOutput, TextEmbeddingsError> {
        let (sender, receiver) = oneshot::channel();

        // ë¸”ë¡œì»¤ B2: ë°±í”„ë ˆì…”ë¥¼ ìœ„í•´ send().await ì‚¬ìš© (ê°€ë“ ì°° ë•Œ íŒ¨ë‹‰í•˜ëŠ” try_send ì•„ë‹˜)
        self.backend
            .backend_sender
            .send(BackendCommand::EmbedListwise(input, Span::current(), sender))
            .await
            .map_err(|e| TextEmbeddingsError::Backend(
                format!("Backend channel closed: {}", e)
            ))?;

        receiver
            .await
            .expect("Backend blocking task dropped the sender without a response. This is a bug.")
            .map_err(TextEmbeddingsError::Backend)
    }

    /// ê¸°ë³¸ í† í¬ë‚˜ì´ì € ì ‘ê·¼ (ë¼ìš°í„° í—¬í¼ì—ì„œ ì‚¬ìš©)
    pub fn tokenizer(&self) -> &Tokenizer {
        self.tokenization.tokenizer()
    }
}
```

### 9.4 í˜ì´ë¡œë“œ ì œí•œ ë ˆì´ì–´

âš ï¸ **í•„ìˆ˜ ìˆ˜ì • 1: AppState ìƒì„± ì „ CLI ARGS ì‚¬ìš©**

HTTP ì„œë²„ ìŠ¤íƒì— RequestBodyLimitLayerë¥¼ ì¶”ê°€í•˜ì—¬ chunked/H2 ìš”ì²­ì— ëŒ€í•´ì„œë„ payload limitì´ ê°•ì œë˜ë„ë¡ í•©ë‹ˆë‹¤.

**ì¤‘ìš” ìˆ˜ì •:** ë¼ìš°í„° ìƒì„±ì€ AppStateë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆê¸° ì „ì— ë°œìƒí•©ë‹ˆë‹¤. `state`ê°€ ì•„ë‹Œ CLI `args`ë¥¼ ì§ì ‘ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

**ì¤‘ìš” ë°°ì¹˜:** **ë¼ìš°íŒ… ë¡œì§ ì „ ìµœìƒìœ„ ë¼ìš°í„°**ì— ì ìš©. ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë“  ë¼ìš°íŠ¸ê°€ ì œí•œì„ ì¤€ìˆ˜í•©ë‹ˆë‹¤.

```rust
use tower_http::limit::RequestBodyLimitLayer;

// í•„ìˆ˜ ìˆ˜ì • 1: ë¼ìš°í„° ë¹Œë“œ ì „ CLI argsì—ì„œ ì œí•œ ì¶”ì¶œ
// ë¼ìš°í„°ëŠ” AppState ì „ì— ìƒì„±ë˜ë¯€ë¡œ state.listwise_configì— ì ‘ê·¼ ë¶ˆê°€
let payload_limit_bytes = args.listwise_payload_limit_bytes as u64;

let app = Router::new()
    // ... ë¼ìš°íŠ¸ ì •ì˜ ...
    // ìµœì™¸ê³½ ë ˆì´ì–´ë¡œ RequestBodyLimitLayer ì ìš© (ë¯¸ë“¤ì›¨ì–´ ìŠ¤íƒì—ì„œ ë¨¼ì € ì‹¤í–‰)
    .layer(RequestBodyLimitLayer::new(payload_limit_bytes));

// ë‚˜ì¤‘ì—: ê°™ì€ args ê°’ì„ ì‚¬ìš©í•˜ì—¬ AppState ìƒì„±
let state = AppState::new(/* ë‚´ë¶€ì ìœ¼ë¡œ args.listwise_payload_limit_bytes ì‚¬ìš© */);
```

> **ì™œ ì¤‘ìš”í•œê°€:** ë¼ìš°í„°ëŠ” TEIì˜ ì´ˆê¸°í™” ì‹œí€€ìŠ¤ì—ì„œ `AppState`ê°€ ì¡´ì¬í•˜ê¸° ì „ì— ë¹Œë“œë©ë‹ˆë‹¤.
> ë¼ìš°í„° ìƒì„± ì‹œì ì— `state.listwise_config.payload_limit_bytes`ì— ì ‘ê·¼í•˜ë ¤ê³  í•˜ë©´
> ì»´íŒŒì¼ ì—ëŸ¬ê°€ ë°œìƒí•˜ê±°ë‚˜ ì–´ìƒ‰í•œ ë¦¬íŒ©í† ë§ì´ í•„ìš”í•©ë‹ˆë‹¤. `args`ë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì˜¬ë°”ë¥¸ íŒ¨í„´ì…ë‹ˆë‹¤.

> **ìµœìƒìœ„ ë°°ì¹˜:** `.layer()` í˜¸ì¶œì€ ë¼ìš°í„° ì²´ì¸ì˜ ë§ˆì§€ë§‰ ë©”ì†Œë“œ(ìµœì™¸ê³½ ë ˆì´ì–´)ì—¬ì•¼ í•˜ë¯€ë¡œ
> ë¯¸ë“¤ì›¨ì–´ ìŠ¤íƒì—ì„œ ë¨¼ì € ì‹¤í–‰ë˜ì–´ ëª¨ë“  ë¼ìš°íŠ¸ì— ê· ì¼í•˜ê²Œ ì ìš©ë©ë‹ˆë‹¤.

### 9.5 ë””ë²„ê¹… ê°€ì´ë“œ

**ì¼ë°˜ì ì¸ ë¹Œë“œ ì—ëŸ¬**

1. `cannot find type 'ErrorType'` â€” ìœ„ì— í‘œì‹œëœ enumì´ `server.rs`ì— ìˆëŠ”ì§€ í™•ì¸.
2. `method 'tokenizer' not found for struct 'Infer'` â€” ì„¹ì…˜ 9.3ì˜ í—¬í¼ ì¶”ê°€.
3. `unresolved import 'text_embeddings_core'` â€” ë¶€ë¡ Aì˜ crate ì´ë¦„ ë³€ê²½ ì ìš©.

**ëŸ°íƒ€ì„ í•¨ì •**

1. `Missing embed_token` â€” listwise ê°ì§€(projector ê°€ì¤‘ì¹˜ + íŠ¹ìˆ˜ í† í°) ì„±ê³µ í™•ì¸.
2. `Block processing timed out` â€” `--listwise-block-timeout-ms` ëŠ˜ë¦¬ê±°ë‚˜ `--max-listwise-docs-per-pass` ë‚®ì¶”ê¸°.

---

## ì˜ì¡´ì„± & Cargo.toml

**íŒŒì¼:** `Cargo.toml` (ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ë£¨íŠ¸ ë˜ëŠ” ê´€ë ¨ íŒ¨í‚¤ì§€)

âš ï¸ **SHOULD-FIX S5: ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ë²„ì „ ì •ë ¬ ì¤‘ìš”**

ì•„ë˜ í‘œì‹œëœ ë²„ì „ì€ ì˜ˆì‹œì…ë‹ˆë‹¤. ì˜ì¡´ì„±ì„ ì¶”ê°€í•˜ê¸° ì „ì— **í•­ìƒ ê¸°ì¡´ TEI ì›Œí¬ìŠ¤í˜ì´ìŠ¤ `Cargo.toml`ì„ í™•ì¸**í•˜ê³  ê±°ê¸°ì— ì§€ì •ëœ ì •í™•í•œ ë²„ì „ì„ ì‚¬ìš©í•˜ì—¬ ì¶©ëŒì„ í”¼í•˜ì„¸ìš”!

```toml
[dependencies]
# Projector ê°€ì¤‘ì¹˜ ê°ì§€ì— í•„ìš” (safetensors í—¤ë” íŒŒì‹±)
# âš ï¸ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ë²„ì „ í™•ì¸! ì˜ˆì‹œëŠ” 0.4ë¥¼ ë³´ì—¬ì£¼ì§€ë§Œ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ëŠ” ë‹¤ë¥¸ ë²„ì „ ì‚¬ìš© ê°€ëŠ¥
safetensors = "0.4"  # ì›Œí¬ìŠ¤í˜ì´ìŠ¤ Cargo.tomlì— ëŒ€í•´ í™•ì¸

# TEIì— ì´ë¯¸ ìˆìŒ - ì¤‘ë³µ í•­ëª© ì¶”ê°€í•˜ì§€ ë§ ê²ƒ!
# ì°¸ì¡°ìš©ìœ¼ë¡œë§Œ í‘œì‹œ - ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ë²„ì „ í™•ì¸:
tokenizers = "0.15"      # í™•ì¸ - ì›Œí¬ìŠ¤í˜ì´ìŠ¤ëŠ” 0.13 ë˜ëŠ” 0.19 ì‚¬ìš© ê°€ëŠ¥
candle-core = "0.4"      # í™•ì¸ - ì›Œí¬ìŠ¤í˜ì´ìŠ¤ëŠ” 0.3 ë˜ëŠ” 0.5 ì‚¬ìš© ê°€ëŠ¥
candle-nn = "0.4"        # candle-core ë²„ì „ê³¼ ì¼ì¹˜í•´ì•¼ í•¨
anyhow = "1.0"           # ì¼ë°˜ì ìœ¼ë¡œ ì•ˆì „í•˜ì§€ë§Œ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ í™•ì¸
tracing = "0.1"          # ì¼ë°˜ì ìœ¼ë¡œ ìˆìŒ, ë²„ì „ í™•ì¸

# HTTP í˜ì´ë¡œë“œ ì œí•œìš©
tower-http = { version = "0.4", features = ["limit"] }  # ë²„ì „ í™•ì¸

# ë©”ëª¨ë¦¬ ë§µ I/Oìš© (safetensors í—¤ë” íŒŒì‹±)
memmap2 = "0.9"  # ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ë²„ì „ í™•ì¸
```

> **ì¤‘ìš” (S5):** TEIëŠ” ì ê¸´ ë²„ì „ì´ ìˆëŠ” ì›Œí¬ìŠ¤í˜ì´ìŠ¤ `Cargo.toml`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ë²„ì „ìœ¼ë¡œ ì˜ì¡´ì„±ì„ ì¶”ê°€í•˜ë©´ ì»´íŒŒì¼ ì‹¤íŒ¨ ë˜ëŠ” ëŸ°íƒ€ì„ ë¹„í˜¸í™˜ì„±ì´ ë°œìƒí•©ë‹ˆë‹¤. ìœ„ì˜ ì˜ì¡´ì„± ì¤„ì„ ë³µì‚¬í•˜ê¸° ì „ì—:
>
> 1. `text-embeddings-inference/Cargo.toml` ì—´ê¸° (ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ë£¨íŠ¸)
> 2. `[workspace.dependencies]` ì„¹ì…˜ í™•ì¸
> 3. ê±°ê¸°ì— ì§€ì •ëœ ì •í™•í•œ ë²„ì „ ì‚¬ìš© (ì˜ˆ: ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì— `candle-core = "0.5"`ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒ ì‚¬ìš©)
> 4. `safetensors`ì˜ ê²½ìš° ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì— ì—†ìœ¼ë©´ ê¸°ì¡´ depsì™€ í˜¸í™˜ë˜ëŠ” ë²„ì „ìœ¼ë¡œ ì¶”ê°€

> **ì£¼ì˜:** `safetensors` crateëŠ” ëª¨ë¸ ê°ì§€(Milestone 1)ì—ì„œ ëª¨ë¸ í—¤ë”ë¥¼ íŒŒì‹±í•˜ê³  projector ê°€ì¤‘ì¹˜ë¥¼ í™•ì¸í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. LBNL rerankerë¥¼ í‘œì¤€ classifierì™€ êµ¬ë³„í•˜ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤.

---

## ë¶€ë¡ A â€“ Crate ì´ë¦„ ë§¤í•‘

ìœ„ì˜ ìŠ¤ë‹ˆí«ì€ ì˜ˆì œë¥¼ ê°„ê²°í•˜ê²Œ ìœ ì§€í•˜ê¸° ìœ„í•´ ë‹¨ìˆœí™”ëœ crate ì ‘ë‘ì‚¬(`text_embeddings_core`, `text_embeddings_backend_core` ë“±)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì»´íŒŒì¼í•˜ê¸° ì „ì— ì‹¤ì œ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ crateì— ë§¤í•‘í•˜ì„¸ìš”:

| ì˜ˆì œ ì ‘ë‘ì‚¬ | TEI ì €ì¥ì†Œì—ì„œ ì‚¬ìš© |
|------------|-------------------|
| `text_embeddings_core` | `text_embeddings_core` |
| `text_embeddings_backend_core` | `text_embeddings_backend_core` |
| `text_embeddings_backend_candle` | `text_embeddings_backend_candle` |
| `router` | `router` |

> íŒ: ìŠ¤ë‹ˆí«ì„ ì½”ë“œë² ì´ìŠ¤ì— ë³µì‚¬í•œ í›„ ëŒ€ìƒ `sed` êµì²´ ì‹¤í–‰ (ì˜ˆ: `sed -i '' 's/text_embeddings_core::/text_embeddings_core::/g'`).

---

## ìƒì„±/ìˆ˜ì •ëœ ì£¼ìš” íŒŒì¼

**ì‹ ê·œ íŒŒì¼:**
- `core/src/prompt.rs`
- `core/src/detection.rs`
- `backends/candle/src/layers/projector.rs`
- `backends/candle/src/models/lbnl_reranker.rs`
- `router/src/listwise/mod.rs`
- `router/src/listwise/math.rs`
- `router/src/strategy.rs`

**ìˆ˜ì •ëœ íŒŒì¼:**
- `backends/core/src/lib.rs` (embed_listwise_blockë¡œ Backend trait í™•ì¥)
- `backends/candle/src/models/qwen3.rs` (hidden state ì¶”ì¶œ)
- `router/src/lib.rs` (ê°ì§€, AppState)
- `core/src/lib.rs` (ëª¨ë“ˆ export)
- `core/src/tokenization.rs` (left padding, validation)
- `router/src/http/server.rs` (listwise í•¸ë“¤ëŸ¬)
- `router/src/prometheus.rs` (ë©”íŠ¸ë¦­)

ëª¨ë“  í•„ìš”í•œ í†µí•© ì§€ì ì´ ì´ì œ ë¬¸ì„œí™”ë˜ì—ˆìŠµë‹ˆë‹¤; ìœ„ì— ì–¸ê¸‰ëœ ëŒ€ë¡œ crate ì ‘ë‘ì‚¬ì™€ ê°€ì¤‘ì¹˜ ê²½ë¡œë¥¼ ì¡°ì •í•œ í›„ `cargo build`ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.

---

## ê²€í†  í”¼ë“œë°± ì ìš© - ë³€ê²½ ë¡œê·¸

ì´ ë²„ì „(v1.4)ì€ ê¸°ìˆ  ê²€í† ì˜ í¬ê´„ì ì¸ í”¼ë“œë°±ì„ í†µí•©í•©ë‹ˆë‹¤. ëª¨ë“  ì¤‘ìš” ì´ìŠˆ, ê¶Œì¥ ìˆ˜ì •ì‚¬í•­ ë° ê°œì„  í•­ëª©ì´ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.

### ë²„ì „ 1.4 ìš”ì•½ í†µê³„

**ê²€í†  ìƒíƒœ:** âœ… **ìŠ¹ì¸ë¨** - ìì‹ ê° ìˆê²Œ ë³‘í•© ì¤€ë¹„ ì™„ë£Œ

**ì ìš©ëœ ìˆ˜ì •:**
- âœ… ì¤‘ìš” ë¸”ë¡œì»¤: 1/1 (ì˜ëª»ëœ ëª¨ë“œ ì¡°í•© ê±°ë¶€)
- âœ… ì¤‘ìš” í•„ìˆ˜ ìˆ˜ì •: 3/3 (ëª¨ë“ˆ export, ë³€ìˆ˜, Dtype ì•ˆì „ì„±)
- âœ… ê³ ê°€ì¹˜ Nit: 3/11 (Pad í† í° ìˆœì„œ, ë©”íŠ¸ë¦­ ë‹¨ìœ„, í† í¬ë‚˜ì´ì œì´ì…˜ ì •ì±…)
- âœ… í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸: í¬ê´„ì ì¸ 6ê°€ì§€ ë²”ì£¼ í…ŒìŠ¤íŠ¸ ê³„íš ë¬¸ì„œí™”
- â³ ë‚¨ì€ Nit: 8ê°œ í•­ëª© (ê°œì„ , ë¹„ì°¨ë‹¨, ì ì§„ì  ì ìš© ê°€ëŠ¥)

**Python ì°¸ì¡° íŒ¨ë¦¬í‹°:** âœ… ì™„ì „íˆ ê²€ì¦ë¨
- âœ… í”„ë¡¬í”„íŠ¸ êµ¬ì¡° & ìƒŒë“œìœ„ì¹˜ íŒ¨í„´
- âœ… Left padding & í† í¬ë‚˜ì´ì œì´ì…˜ ì •ì±…
- âœ… ì ˆë‹¨ (512 ì¿¼ë¦¬, 2048 ë¬¸ì„œ) + ë””ì½”ë“œ
- âœ… ë¸”ë¡ ì²­í‚¹ (125 ìµœëŒ€, ìš©ëŸ‰ ê¸°ë°˜)
- âœ… Projector ì•„í‚¤í…ì²˜ (1024â†’512â†’512, ReLU, bias ì—†ìŒ)
- âœ… ê°€ì¤‘ í‰ê·  ê³µì‹: `(Î£ wÂ·z) / Î£w`
- âœ… ìµœì¢… ì ìˆ˜: cosine(combined_query, all_docs)

**ì»´íŒŒì¼ ì•ˆì „ì„±:** âœ… ëª¨ë“  ë¸”ë¡œì»¤ í•´ê²°ë¨
- âœ… ëª¨ë“  ëª¨ë“ˆ export ìˆìŒ (tokenization, prompt, detection)
- âœ… ëª¨ë“  ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥¸ í•„ë“œ ì°¸ì¡°
- âœ… Dtype ë¶ˆì¼ì¹˜ ì—†ìŒ (Batch ê²½ë¡œ ê°•ì œ)
- âœ… ì‹œì‘ì‹œ ì˜ëª»ëœ ëª¨ë“œ ì¡°í•© ê±°ë¶€ (ë¸”ë¡œì»¤ ìˆ˜ì •)

**ëŸ°íƒ€ì„ ì•ˆì „ì„±:** âœ… í”„ë¡œë•ì…˜ ë“±ê¸‰
- âœ… ë°±ì—”ë“œ ë°±í”„ë ˆì…” (send().await, try_send ì•„ë‹˜)
- âœ… íƒ€ì„ì•„ì›ƒ ë¹„ì·¨ì†Œ ë¬¸ì„œí™”
- âœ… íŠ¹ìˆ˜ í† í° ê²€ì¦ ì ìš©
- âœ… í˜ì´ë¡œë“œ ì œí•œ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •
- âœ… Strategy ê²€ì¦ì´ 5xx ì—ëŸ¬ ë°©ì§€ (ë¸”ë¡œì»¤ ìˆ˜ì •)

**ìš´ì˜ í’ˆì§ˆ:** âœ… í–¥ìƒë¨
- âœ… ëŒ€ì‹œë³´ë“œ ëª…í™•ì„±ì„ ìœ„í•œ ë©”íŠ¸ë¦­ ë‹¨ìœ„ ë¬¸ì„œí™”
- âœ… Pad í† í° í´ë°± ìˆœì„œê°€ Python ì°¸ì¡°ì™€ ì¼ì¹˜
- âœ… í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ (6ê°€ì§€ ë²”ì£¼, 10ë¶„ ì„¤ì •)
- âœ… ì˜ëª»ëœ êµ¬ì„±ì— ëŒ€í•œ ëª…í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€

---

## ğŸ‰ êµ¬í˜„ ì¤€ë¹„ ì™„ë£Œ

**ê²€í† ì í‰ê²°:** *"í•œ ë¸”ë¡œì»¤ + ëª‡ ê°€ì§€ nitë¡œ ìŠ¹ì¸"* â†’ **ë¸”ë¡œì»¤ ìˆ˜ì •ë¨, ìŠ¹ì¸ë¨**

ê³„íšì€ ë‹¤ìŒê³¼ í•¨ê»˜ **í”„ë¡œë•ì…˜ ì¤€ë¹„ ì™„ë£Œ**:
- âœ… **ë¸”ë¡œì»¤ í•´ê²°ë¨:** ì˜ëª»ëœ ëª¨ë“œ ì¡°í•©ì´ ì´ì œ ëª…í™•í•œ ì—ëŸ¬ë¡œ ê±°ë¶€ë¨
- âœ… ì»´íŒŒì¼ ë¸”ë¡œì»¤ ì—†ìŒ (ëª¨ë“  ëª¨ë“ˆ exportë¨, ë³€ìˆ˜ ì •í™•)
- âœ… ì¤‘ìš” ëŸ°íƒ€ì„ ë¸”ë¡œì»¤ ì—†ìŒ (dtype ì•ˆì „ì„±, strategy ê²€ì¦)
- âœ… ì™„ì „í•œ Qwen3 hidden states êµ¬í˜„ (forward_layers ì¶”ì¶œ)
- âœ… ë°±ì—”ë“œ ì±„ë„ ë°±í”„ë ˆì…” (ë¡œë“œ ì¤‘ íŒ¨ë‹‰ ì—†ìŒ)
- âœ… í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸ ê³„íš (strategy, ì²­í‚¹, íŒ¨ë¦¬í‹°, ì—£ì§€ ì¼€ì´ìŠ¤)
- âœ… Python ì°¸ì¡°ì™€ ìˆ˜ì¹˜ íŒ¨ë¦¬í‹° ê²€ì¦ë¨

**ë‹¤ìŒ ë‹¨ê³„:**

1. **ë¸”ë¡œì»¤ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹¤í–‰:**
   ```bash
   # ì¤‘ìš”: ë¸”ë¡œì»¤ ìˆ˜ì • ì‘ë™ í™•ì¸
   ./tei --model jinaai/jina-reranker-v3 --reranker-mode pairwise
   # ì˜ˆìƒ: listwise ì „ìš© ëª¨ë¸ì— ëŒ€í•œ ë©”ì‹œì§€ì™€ í•¨ê»˜ ì¦‰ì‹œ ì—ëŸ¬
   ```

2. **ë³‘í•© ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
   ```bash
   cargo fmt
   cargo clippy --all --all-targets --all-features -- --deny warnings
   cargo test --all
   ```

3. **Milestone ìˆœì„œ ë”°ë¥´ê¸°:**
    - Milestone 1ë¶€í„° ì‹œì‘ (ëª¨ë¸ ê°ì§€ & CLI)
    - 9ê°œ Milestone ëª¨ë‘ ìˆœì°¨ì ìœ¼ë¡œ ì§„í–‰
    - íŠ¹ì • êµ¬í˜„ì„ ìœ„í•´ ë³€ê²½ ë¡œê·¸ì˜ ì¤„ ë²ˆí˜¸ ì°¸ì¡°

**êµ¬í˜„ ìŠ¹ì¸ë¨ - ì‹œì‘ ì¤€ë¹„!** ğŸš€